{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(DATA_PATHS['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(DATA_PATHS['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(DATA_PATHS['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Input shape:\t(1855, 172)\n",
      "After standardization (1855, 172)\n",
      "After converting categoricals:\t(1855, 498)\n",
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Input shape:\t(3255, 221)\n",
      "After standardization (3255, 221)\n",
      "After converting categoricals:\t(3255, 936)\n",
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Input shape:\t(6469, 82)\n",
      "After standardization (6469, 82)\n",
      "After converting categoricals:\t(6469, 564)\n"
     ]
    }
   ],
   "source": [
    "# Filter out columns with low entropy\n",
    "print(\"Country A\")\n",
    "a_train_reduc = filter_columns(a_train.drop('poor', axis=1))\n",
    "aX_train = pre_process_data(a_train_reduc)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train_reduc = filter_columns(b_train.drop('poor', axis=1))\n",
    "bX_train = pre_process_data(b_train_reduc)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train_reduc = filter_columns(c_train.drop('poor', axis=1))\n",
    "cX_train = pre_process_data(c_train_reduc)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD  # PCA\n",
    "\n",
    "def reduce_dimensions(x):\n",
    "    #pca = PCA(n_components=40)\n",
    "    svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "    return svd.fit_transform(x), svd\n",
    "\n",
    "# reduce dimensions for all countries\n",
    "aX_train_svd, a_svd = reduce_dimensions(aX_train)\n",
    "bX_train_svd, b_svd = reduce_dimensions(bX_train)\n",
    "cX_train_svd, c_svd = reduce_dimensions(cX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train\n",
    "test_size = 0. # No CV for submissions\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train_svd, ay_train, test_size=test_size, xgb_format=True)\n",
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train_svd, by_train, test_size=test_size, xgb_format=True)\n",
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train_svd, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 8000\n",
    "params = {'max_depth': 20, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)\n",
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)\n",
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 0.2744477151049117 - Test: None\n",
      "B Loss. Train: 0.17159917608951814 - Test: None\n",
      "C Loss. Train: 0.1456253027320118 - Test: None\n"
     ]
    }
   ],
   "source": [
    "# With PCA reduction (40 dims): 0.7444784675249769\n",
    "# Without reduction: 0.7105216116701498\n",
    "# With SVD reduction: 0.585940340593645\n",
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n",
      "Input shape:\t(1604, 221)\n",
      "After standardization (1604, 221)\n",
      "After converting categoricals:\t(1604, 925)\n",
      "Input shape:\t(3187, 82)\n",
      "After standardization (3187, 82)\n",
      "After converting categoricals:\t(3187, 547)\n",
      "Submission saved.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Submission\n",
    "# TODO: tidy this up\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(DATA_PATHS['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(DATA_PATHS['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(DATA_PATHS['C']['test'], index_col='id')\n",
    "\n",
    "# columns to keep from test data\n",
    "a_keep = a_train_reduc.columns.tolist()\n",
    "b_keep = b_train_reduc.columns.tolist()\n",
    "c_keep = c_train_reduc.columns.tolist()\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "# Create dummies, standarize numeric values\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "# Reduce dimensions (comment if not testing this)\n",
    "a_test_svd = a_svd.transform(a_test)\n",
    "b_test_svd = b_svd.transform(b_test)\n",
    "c_test_svd = c_svd.transform(c_test)\n",
    "\n",
    "# Create XGBoost matrix\n",
    "a_testxgb = xgb.DMatrix(a_test_svd)\n",
    "b_testxgb = xgb.DMatrix(b_test_svd)\n",
    "c_testxgb = xgb.DMatrix(c_test_svd)\n",
    "\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "# Prepare dataframes for each country\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "submission.to_csv('submission_xgb.csv')\n",
    "print(\"Submission saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the optimal hyperparameters for each country\n",
    "def tune_params(dtrain, dtest, y_test):\n",
    "    params = {\n",
    "        'max_depth': 20,\n",
    "        'eta': 0.05, \n",
    "        'silent': 0,\n",
    "        'lambda': 0.5,\n",
    "        'alpha': 0.5,\n",
    "        'lambda_bias': 0.5, \n",
    "        'min_child_weight': 1,\n",
    "        'objective': 'binary:logistic', \n",
    "        'eval_metric': 'logloss', \n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    current_loss = 10000\n",
    "    best_num_rounds = 0\n",
    "    best_hyperparams = {}\n",
    "    num_rounds = [1000, 3000, 5000, 8000]\n",
    "    max_depths = list(range(3, 15))\n",
    "    etas = [0.01, 0.05]\n",
    "    min_child_weights = [1, 2]\n",
    "    lamdas = [0.5, 1]\n",
    "    alphas = [0.5, 1]\n",
    "    lambda_biases = [0.5, 1]\n",
    "    total_combinations = len(num_rounds) * len(max_depths) * len(etas)*\\\n",
    "        len(min_child_weights) * len(lamdas) * len(alphas) * len(lambda_biases)\n",
    "\n",
    "    with tqdm(total=total_combinations) as pbar:\n",
    "        for num_round in num_rounds:\n",
    "            for max_depth in max_depths:\n",
    "                for eta in etas:\n",
    "                    for min_child_weight in min_child_weights:\n",
    "                        for lamda in lamdas:\n",
    "                            for alpha in alphas:\n",
    "                                for lambda_bias in lambda_biases:\n",
    "                                    params['max_depth'] = max_depth\n",
    "                                    params['eta'] = eta\n",
    "                                    params['min_child_weight'] = min_child_weight\n",
    "                                    params['lambda'] = lamda\n",
    "                                    params['alpha'] = alpha\n",
    "                                    params['lambda_bias'] = lambda_bias\n",
    "\n",
    "                                    model = train_xgb_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "                                    pred = model.predict(dtest)\n",
    "                                    loss = log_loss(pred, y_test)\n",
    "\n",
    "                                    if loss < current_loss:\n",
    "                                        current_loss = loss\n",
    "                                        best_hyperparams = params\n",
    "                                        best_num_rounds = num_round\n",
    "                                    pbar.update(1)\n",
    "\n",
    "    return best_hyperparams, best_num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1536 [02:13<5:37:51, 13.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-811916256868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Wont use Cross validation in this one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_num_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_ax_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_ax_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_ay_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-864f3dbef0d7>\u001b[0m in \u001b[0;36mtune_params\u001b[0;34m(dtrain, dtest, y_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda_bias'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/poverT/exp/utils.py\u001b[0m in \u001b[0;36mtrain_xgb_model\u001b[0;34m(dtrain, params, num_round)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'silent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'objective'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'reg:logistic'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Wont use Cross validation in this one\n",
    "a_params, a_num_rounds = tune_params(xgb_ax_train, xgb_ax_train, xgb_ay_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_params, b_num_rounds = tune_params(xgb_bx_train, xgb_bx_test, xgb_by_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_params, c_num_rounds = tune_params(xgb_cx_train, xgb_cx_test, xgb_cy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
