{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(DATA_PATHS['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(DATA_PATHS['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(DATA_PATHS['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Input shape:\t(1855, 172)\n",
      "After standardization (1855, 172)\n",
      "After converting categoricals:\t(1855, 498)\n",
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Input shape:\t(3255, 221)\n",
      "After standardization (3255, 221)\n",
      "After converting categoricals:\t(3255, 936)\n",
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Input shape:\t(6469, 82)\n",
      "After standardization (6469, 82)\n",
      "After converting categoricals:\t(6469, 564)\n"
     ]
    }
   ],
   "source": [
    "# Filter out columns with low entropy\n",
    "print(\"Country A\")\n",
    "a_train_reduc = filter_columns(a_train.drop('poor', axis=1))\n",
    "aX_train = pre_process_data(a_train_reduc)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train_reduc = filter_columns(b_train.drop('poor', axis=1))\n",
    "bX_train = pre_process_data(b_train_reduc)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train_reduc = filter_columns(c_train.drop('poor', axis=1))\n",
    "cX_train = pre_process_data(c_train_reduc)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD  # PCA\n",
    "\n",
    "def reduce_dimensions(x, n_comp=100):\n",
    "    #pca = PCA(n_components=40)\n",
    "    svd = TruncatedSVD(n_components=n_comp, n_iter=7, random_state=42)\n",
    "    return svd.fit_transform(x), svd\n",
    "\n",
    "# reduce dimensions for all countries\n",
    "aX_train_svd, a_svd = reduce_dimensions(aX_train)\n",
    "bX_train_svd, b_svd = reduce_dimensions(bX_train)\n",
    "cX_train_svd, c_svd = reduce_dimensions(cX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train\n",
    "test_size = 0.2\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train_svd, ay_train, test_size=test_size, xgb_format=True)\n",
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train_svd, by_train, test_size=test_size, xgb_format=True)\n",
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train_svd, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 8000\n",
    "params = {'max_depth': 20, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)\n",
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)\n",
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 0.2744477151049117 - Test: None\n",
      "B Loss. Train: 0.17159917608951814 - Test: None\n",
      "C Loss. Train: 0.1456253027320118 - Test: None\n"
     ]
    }
   ],
   "source": [
    "# With PCA reduction (40 dims): 0.7444784675249769\n",
    "# Without reduction: 0.7105216116701498\n",
    "# With SVD reduction: 0.585940340593645\n",
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n",
      "Input shape:\t(1604, 221)\n",
      "After standardization (1604, 221)\n",
      "After converting categoricals:\t(1604, 925)\n",
      "Input shape:\t(3187, 82)\n",
      "After standardization (3187, 82)\n",
      "After converting categoricals:\t(3187, 547)\n",
      "Submission saved.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Submission\n",
    "# TODO: tidy this up\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(DATA_PATHS['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(DATA_PATHS['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(DATA_PATHS['C']['test'], index_col='id')\n",
    "\n",
    "# columns to keep from test data\n",
    "a_keep = a_train_reduc.columns.tolist()\n",
    "b_keep = b_train_reduc.columns.tolist()\n",
    "c_keep = c_train_reduc.columns.tolist()\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "# Create dummies, standarize numeric values\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "# Reduce dimensions (comment if not testing this)\n",
    "a_test_svd = a_svd.transform(a_test)\n",
    "b_test_svd = b_svd.transform(b_test)\n",
    "c_test_svd = c_svd.transform(c_test)\n",
    "\n",
    "# Create XGBoost matrix\n",
    "a_testxgb = xgb.DMatrix(a_test_svd)\n",
    "b_testxgb = xgb.DMatrix(b_test_svd)\n",
    "c_testxgb = xgb.DMatrix(c_test_svd)\n",
    "\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "# Prepare dataframes for each country\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "submission.to_csv('submission_xgb.csv')\n",
    "print(\"Submission saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the optimal hyperparameters for each country\n",
    "def tune_params(dtrain, dtest, y_test):\n",
    "    params = {\n",
    "        'max_depth': 20,\n",
    "        'eta': 0.05, \n",
    "        'silent': 0,\n",
    "        'lambda': 0.5,\n",
    "        'alpha': 0.5,\n",
    "        'lambda_bias': 0.5, \n",
    "        'min_child_weight': 1,\n",
    "        'objective': 'binary:logistic', \n",
    "        'eval_metric': 'logloss', \n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    current_loss = 10000\n",
    "    best_num_rounds = 0\n",
    "    best_hyperparams = {}\n",
    "    num_rounds = [1000, 3000, 5000, 8000]\n",
    "    max_depths = list(range(3, 15))\n",
    "    etas = [0.01, 0.05]\n",
    "    min_child_weights = [1, 2]\n",
    "    lamdas = [0.5, 1]\n",
    "    alphas = [0.5, 1]\n",
    "    lambda_biases = [0.5, 1]\n",
    "    total_combinations = len(num_rounds) * len(max_depths) * len(etas)*\\\n",
    "        len(min_child_weights) * len(lamdas) * len(alphas) * len(lambda_biases)\n",
    "\n",
    "    with tqdm(total=total_combinations) as pbar:\n",
    "        for num_round in num_rounds:\n",
    "            for max_depth in max_depths:\n",
    "                for eta in etas:\n",
    "                    for min_child_weight in min_child_weights:\n",
    "                        for lamda in lamdas:\n",
    "                            for alpha in alphas:\n",
    "                                for lambda_bias in lambda_biases:\n",
    "                                    params['max_depth'] = max_depth\n",
    "                                    params['eta'] = eta\n",
    "                                    params['min_child_weight'] = min_child_weight\n",
    "                                    params['lambda'] = lamda\n",
    "                                    params['alpha'] = alpha\n",
    "                                    params['lambda_bias'] = lambda_bias\n",
    "\n",
    "                                    model = train_xgb_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "                                    pred = model.predict(dtest)\n",
    "                                    loss = log_loss(pred, y_test)\n",
    "\n",
    "                                    if loss < current_loss:\n",
    "                                        current_loss = loss\n",
    "                                        best_hyperparams = params\n",
    "                                        best_num_rounds = num_round\n",
    "                                    pbar.update(1)\n",
    "\n",
    "    return best_hyperparams, best_num_rounds, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 124/1536 [09:25<1:30:42,  3.85s/it]"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "best_params = {}\n",
    "best_rounds = 0\n",
    "best_loss = 1000\n",
    "best_svd = None\n",
    "for n in [50, 100, 200]:\n",
    "    aX_train_svd, a_svd = reduce_dimensions(aX_train, n_comp=n)\n",
    "    xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train_svd, ay_train, test_size=test_size, xgb_format=True)\n",
    "    a_params, a_num_rounds, loss = tune_params(xgb_ax_train, xgb_ax_test, xgb_ay_test)\n",
    "    if loss < best_loss:\n",
    "        best_params = a_params\n",
    "        best_rounds = a_num_rounds\n",
    "        best_loss = loss\n",
    "        best_svd = a_svd\n",
    "\n",
    "print(\"A best params for loss: {} :\".format(best_loss))\n",
    "print(best_params)\n",
    "print(best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_params, b_num_rounds = tune_params(xgb_bx_train, xgb_bx_test, xgb_by_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_params, c_num_rounds = tune_params(xgb_cx_train, xgb_cx_test, xgb_cy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
