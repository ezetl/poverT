{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wBXbHZmp</th>\n",
       "      <th>SlDKnCuu</th>\n",
       "      <th>KAJOWiiw</th>\n",
       "      <th>DsKacCdL</th>\n",
       "      <th>rtPrBBPl</th>\n",
       "      <th>tMJrvvut</th>\n",
       "      <th>jdetlNNF</th>\n",
       "      <th>maLAYXwi</th>\n",
       "      <th>vwpsXRGk</th>\n",
       "      <th>sArDRIyX</th>\n",
       "      <th>...</th>\n",
       "      <th>sDGibZrP</th>\n",
       "      <th>CsGvKKBJ</th>\n",
       "      <th>OLpGAaEu</th>\n",
       "      <th>LrDrWRjC</th>\n",
       "      <th>JCDeZBXq</th>\n",
       "      <th>HGPWuGlV</th>\n",
       "      <th>GDUPaBQs</th>\n",
       "      <th>WuwrCsIY</th>\n",
       "      <th>AlDbXTlZ</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46107</th>\n",
       "      <td>JhtDR</td>\n",
       "      <td>GUusz</td>\n",
       "      <td>TuovO</td>\n",
       "      <td>ZYabk</td>\n",
       "      <td>feupP</td>\n",
       "      <td>PHMVg</td>\n",
       "      <td>NDTCU</td>\n",
       "      <td>cLAGr</td>\n",
       "      <td>XAmOF</td>\n",
       "      <td>MwLvg</td>\n",
       "      <td>...</td>\n",
       "      <td>JqHnW</td>\n",
       "      <td>MaXfS</td>\n",
       "      <td>etZsD</td>\n",
       "      <td>idRwx</td>\n",
       "      <td>LPtkN</td>\n",
       "      <td>vkbkA</td>\n",
       "      <td>qQxrL</td>\n",
       "      <td>AITFl</td>\n",
       "      <td>aQeIm</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82739</th>\n",
       "      <td>JhtDR</td>\n",
       "      <td>GUusz</td>\n",
       "      <td>TuovO</td>\n",
       "      <td>ZYabk</td>\n",
       "      <td>feupP</td>\n",
       "      <td>PHMVg</td>\n",
       "      <td>NDTCU</td>\n",
       "      <td>sehIp</td>\n",
       "      <td>lwCkE</td>\n",
       "      <td>MwLvg</td>\n",
       "      <td>...</td>\n",
       "      <td>JqHnW</td>\n",
       "      <td>MaXfS</td>\n",
       "      <td>HxnJy</td>\n",
       "      <td>idRwx</td>\n",
       "      <td>UyAms</td>\n",
       "      <td>vkbkA</td>\n",
       "      <td>qQxrL</td>\n",
       "      <td>AITFl</td>\n",
       "      <td>cecIq</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>JhtDR</td>\n",
       "      <td>GUusz</td>\n",
       "      <td>BIZns</td>\n",
       "      <td>ZYabk</td>\n",
       "      <td>uxuSS</td>\n",
       "      <td>PHMVg</td>\n",
       "      <td>NDTCU</td>\n",
       "      <td>sehIp</td>\n",
       "      <td>qNABl</td>\n",
       "      <td>MwLvg</td>\n",
       "      <td>...</td>\n",
       "      <td>JqHnW</td>\n",
       "      <td>MaXfS</td>\n",
       "      <td>USRak</td>\n",
       "      <td>idRwx</td>\n",
       "      <td>UyAms</td>\n",
       "      <td>vkbkA</td>\n",
       "      <td>qQxrL</td>\n",
       "      <td>AITFl</td>\n",
       "      <td>cecIq</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>JhtDR</td>\n",
       "      <td>GUusz</td>\n",
       "      <td>TuovO</td>\n",
       "      <td>ZYabk</td>\n",
       "      <td>feupP</td>\n",
       "      <td>PHMVg</td>\n",
       "      <td>NDTCU</td>\n",
       "      <td>sehIp</td>\n",
       "      <td>sPNOc</td>\n",
       "      <td>MwLvg</td>\n",
       "      <td>...</td>\n",
       "      <td>JqHnW</td>\n",
       "      <td>MaXfS</td>\n",
       "      <td>USRak</td>\n",
       "      <td>idRwx</td>\n",
       "      <td>UyAms</td>\n",
       "      <td>vkbkA</td>\n",
       "      <td>qQxrL</td>\n",
       "      <td>AITFl</td>\n",
       "      <td>cecIq</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16463</th>\n",
       "      <td>JhtDR</td>\n",
       "      <td>alLXR</td>\n",
       "      <td>TuovO</td>\n",
       "      <td>ZYabk</td>\n",
       "      <td>feupP</td>\n",
       "      <td>PHMVg</td>\n",
       "      <td>NDTCU</td>\n",
       "      <td>cLAGr</td>\n",
       "      <td>NdlDR</td>\n",
       "      <td>MwLvg</td>\n",
       "      <td>...</td>\n",
       "      <td>JqHnW</td>\n",
       "      <td>MaXfS</td>\n",
       "      <td>etZsD</td>\n",
       "      <td>idRwx</td>\n",
       "      <td>UyAms</td>\n",
       "      <td>vkbkA</td>\n",
       "      <td>qQxrL</td>\n",
       "      <td>GAZGl</td>\n",
       "      <td>aQeIm</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 345 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wBXbHZmp SlDKnCuu KAJOWiiw DsKacCdL rtPrBBPl tMJrvvut jdetlNNF maLAYXwi  \\\n",
       "id                                                                              \n",
       "46107    JhtDR    GUusz    TuovO    ZYabk    feupP    PHMVg    NDTCU    cLAGr   \n",
       "82739    JhtDR    GUusz    TuovO    ZYabk    feupP    PHMVg    NDTCU    sehIp   \n",
       "9646     JhtDR    GUusz    BIZns    ZYabk    uxuSS    PHMVg    NDTCU    sehIp   \n",
       "10975    JhtDR    GUusz    TuovO    ZYabk    feupP    PHMVg    NDTCU    sehIp   \n",
       "16463    JhtDR    alLXR    TuovO    ZYabk    feupP    PHMVg    NDTCU    cLAGr   \n",
       "\n",
       "      vwpsXRGk sArDRIyX   ...   sDGibZrP CsGvKKBJ OLpGAaEu LrDrWRjC JCDeZBXq  \\\n",
       "id                        ...                                                  \n",
       "46107    XAmOF    MwLvg   ...      JqHnW    MaXfS    etZsD    idRwx    LPtkN   \n",
       "82739    lwCkE    MwLvg   ...      JqHnW    MaXfS    HxnJy    idRwx    UyAms   \n",
       "9646     qNABl    MwLvg   ...      JqHnW    MaXfS    USRak    idRwx    UyAms   \n",
       "10975    sPNOc    MwLvg   ...      JqHnW    MaXfS    USRak    idRwx    UyAms   \n",
       "16463    NdlDR    MwLvg   ...      JqHnW    MaXfS    etZsD    idRwx    UyAms   \n",
       "\n",
       "      HGPWuGlV GDUPaBQs WuwrCsIY AlDbXTlZ country  \n",
       "id                                                 \n",
       "46107    vkbkA    qQxrL    AITFl    aQeIm       A  \n",
       "82739    vkbkA    qQxrL    AITFl    cecIq       A  \n",
       "9646     vkbkA    qQxrL    AITFl    cecIq       A  \n",
       "10975    vkbkA    qQxrL    AITFl    cecIq       A  \n",
       "16463    vkbkA    qQxrL    GAZGl    aQeIm       A  \n",
       "\n",
       "[5 rows x 345 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(a):\n",
    "    return - sum( (a / sum(a)) * np.log((a / sum(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy values: 0.3829585850912254 Â± 0.3739404708672668. Median: 0.3257934680141318\n"
     ]
    }
   ],
   "source": [
    "# Lets find out all the columns whose values are almost monopolized by a single category. That is, whose values belong\n",
    "# 60% (random decision TBH) of the time to the same category.\n",
    "# TODO: in the future measure the entropy of the values and delete those with low entropy\n",
    "df = a_train\n",
    "entropies = []\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    entropies.append(entropy(res.values))\n",
    "    #if max(res.values) > len(df) * 0.7:\n",
    "    #    to_del.append(col)\n",
    "    #if entr < 0.4:\n",
    "    #    to_del.append(col)\n",
    "    \n",
    "avg_entr = np.mean(entropies)\n",
    "std_entr = np.std(entropies)\n",
    "median_entr = np.median(entropies)\n",
    "\n",
    "print(\"Entropy values: {} Â± {}. Median: {}\".format(avg_entr, std_entr, median_entr))\n",
    "\n",
    "# Delete all the columns whose entropy is below average\n",
    "to_del = []\n",
    "for i, col in enumerate(df.columns.tolist()):\n",
    "    if entropies[i] < median_entr:\n",
    "        to_del.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 345. To delete: 172\n"
     ]
    }
   ],
   "source": [
    "print(\"Total columns: {}. To delete: {}\".format(len(df.columns.tolist()), len(to_del)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets delete all the columns we found to be useless\n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "#print(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(1855, 173)\n",
      "After standardization (1855, 173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting categoricals:\t(1855, 500)\n",
      "Shapes: df_train (1855, 500) - dfy_train (1855,)\n"
     ]
    }
   ],
   "source": [
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "print(\"Shapes: df_train {} - dfy_train {}\".format(df_train.shape, dfy_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x, y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "def train_model(dtrain, params=None, num_round=100):\n",
    "    if params is None:\n",
    "        params = {'max_depth': 4, 'eta': 100, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    return bst\n",
    "\n",
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in df_train.columns.tolist():\n",
    "#    print(df_train[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain, dtest, y_train, y_test = prepare_data(df_train, dfy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Test: 0.09433563748211932 - Train: 0.09450553205034791\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "params = {'max_depth': 5, 'eta': 0.05, 'silent': 0, 'lambda': 2, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "num_round = 3000\n",
    "\n",
    "model = train_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "pred = model.predict(dtest)\n",
    "pred_train = model.predict(dtrain)\n",
    "\n",
    "test_loss = log_loss(pred, y_test)\n",
    "train_loss = log_loss(pred_train, y_train)\n",
    "\n",
    "print(\"Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with other 2 countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy values: 0.5983799488688347 Â± 0.5441820751439844 . Median: 0.507305083138068\n",
      "Input shape:\t(3255, 84)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After standardization (3255, 84)\n",
      "After converting categoricals:\t(3255, 519)\n",
      "B Loss Test: 2.7546789167222885 - Train: 0.27297252216582646\n"
     ]
    }
   ],
   "source": [
    "df = b_train\n",
    "to_del = []\n",
    "#for col in df.columns.tolist():\n",
    "#    res = df[col].value_counts()\n",
    "#    if max(res.values) > len(df) * 0.5:\n",
    "#        to_del.append(col)\n",
    "\n",
    "\n",
    "entropies = []\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    entropies.append(entropy(res.values))\n",
    "\n",
    "avg_entr = np.mean(entropies)\n",
    "std_entr = np.std(entropies)\n",
    "median_entr = np.median(entropies)\n",
    "print(\"Entropy values: {} Â± {} . Median: {}\".format(avg_entr, std_entr, median_entr))\n",
    "\n",
    "# Delete all the columns whose entropy is below average\n",
    "to_del = []\n",
    "for i, col in enumerate(df.columns.tolist()):\n",
    "    if entropies[i] < 0.9:\n",
    "        to_del.append(col)\n",
    "\n",
    "\n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "def reduce_dimensions(x):\n",
    "    svd = TruncatedSVD(n_components=200, n_iter=10, random_state=42)\n",
    "    return svd.fit_transform(x)\n",
    "df_train = reduce_dimensions(df_train)\n",
    "\n",
    "\n",
    "dtrain, dtest, y_train, y_test = prepare_data(df_train, dfy_train)\n",
    "params = {'max_depth': 15, 'eta': 0.01, 'silent': 0, 'lambda': 1, 'alpha': 0.5, 'lambda_bias': 0, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "num_round = 3000\n",
    "\n",
    "model = train_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "pred = model.predict(dtest)\n",
    "pred_train = model.predict(dtrain)\n",
    "\n",
    "test_loss = log_loss(pred, y_test)\n",
    "train_loss = log_loss(pred_train, y_train)\n",
    "\n",
    "print(\"B Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(6469, 97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After standardization (6469, 97)\n",
      "After converting categoricals:\t(6469, 557)\n",
      "C Loss Test: 0.17188059898469973 - Train: 0.10635317006104153\n"
     ]
    }
   ],
   "source": [
    "df = c_train\n",
    "to_del = []\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    if max(res.values) > len(df) * 0.7:\n",
    "        to_del.append(col)\n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "dtrain, dtest, y_train, y_test = prepare_data(df_train, dfy_train)\n",
    "params = {'max_depth': 5, 'eta': 0.05, 'silent': 0, 'lambda': 2, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "num_round = 3000\n",
    "\n",
    "model = train_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "pred = model.predict(dtest)\n",
    "pred_train = model.predict(dtrain)\n",
    "\n",
    "test_loss = log_loss(pred, y_test)\n",
    "train_loss = log_loss(pred_train, y_train)\n",
    "\n",
    "print(\"C Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950490617475\n"
     ]
    }
   ],
   "source": [
    "avg_loss = np.mean([0.17188059898469973, 2.585255615957681, 0.09433563748211932])\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try the same with random forests. TL;DR: they overfit a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_rf_model(features, labels, **kwargs):\n",
    "    \n",
    "    # instantiate model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "    \n",
    "    # train model\n",
    "    model.fit(features, labels)\n",
    "    \n",
    "    # get a (not-very-useful) sense of performance\n",
    "    accuracy = model.score(features, labels)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(3255, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After standardization (3255, 28)\n",
      "After converting categoricals:\t(3255, 180)\n",
      "In-sample accuracy: 100.00%\n",
      "B Loss Test: 2.2018298068909394 - Train: 9.992007221626413e-16\n"
     ]
    }
   ],
   "source": [
    "df = b_train\n",
    "to_del = []\n",
    "\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    if max(res.values) > len(df) * 0.4:\n",
    "        to_del.append(col)\n",
    "        \n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train, dfy_train, test_size=0.40, random_state=42)\n",
    "\n",
    "model = train_rf_model(X_train, Y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds_train = model.predict(X_train)\n",
    "test_loss = log_loss(preds, Y_test)\n",
    "train_loss = log_loss(preds_train, Y_train)\n",
    "\n",
    "print(\"B Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(1855, 32)\n",
      "After standardization (1855, 32)\n",
      "After converting categoricals:\t(1855, 166)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 100.00%\n",
      "A Loss Test: 0.23274539132651606 - Train: 9.992007221626415e-16\n"
     ]
    }
   ],
   "source": [
    "df = a_train\n",
    "to_del = []\n",
    "\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    if max(res.values) > len(df) * 0.6:\n",
    "        to_del.append(col)\n",
    "        \n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train, dfy_train, test_size=0.40, random_state=42)\n",
    "\n",
    "model = train_rf_model(X_train, Y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds_train = model.predict(X_train)\n",
    "test_loss = log_loss(preds, Y_test)\n",
    "train_loss = log_loss(preds_train, Y_train)\n",
    "\n",
    "print(\"A Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(6469, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ezetl/miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After standardization (6469, 78)\n",
      "After converting categoricals:\t(6469, 486)\n",
      "In-sample accuracy: 100.00%\n",
      "A Loss Test: 0.5694306195569548 - Train: 9.992007221626415e-16\n"
     ]
    }
   ],
   "source": [
    "df = c_train\n",
    "to_del = []\n",
    "\n",
    "for col in df.columns.tolist():\n",
    "    res = df[col].value_counts()\n",
    "    if max(res.values) > len(df) * 0.6:\n",
    "        to_del.append(col)\n",
    "        \n",
    "to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "df_reduced = df[list(to_keep)]\n",
    "df_train = pre_process_data(df_reduced)\n",
    "df_train.fillna(0, inplace=True)\n",
    "df.fillna(False, inplace=True)\n",
    "dfy_train = np.ravel(df.poor.astype(int))\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_train, dfy_train, test_size=0.30, random_state=42)\n",
    "\n",
    "model = train_rf_model(X_train, Y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "preds_train = model.predict(X_train)\n",
    "test_loss = log_loss(preds, Y_test)\n",
    "train_loss = log_loss(preds_train, Y_train)\n",
    "\n",
    "print(\"A Loss Test: {} - Train: {}\".format(test_loss, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
