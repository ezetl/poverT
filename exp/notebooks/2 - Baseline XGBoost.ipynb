{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Input shape:\t(1855, 344)\n",
      "After standardization (1855, 344)\n",
      "After converting categoricals:\t(1855, 849)\n",
      "Shapes: aX_train (1855, 849) - ay_train (1855,)\n",
      "\n",
      "Country B\n",
      "Input shape:\t(3255, 441)\n",
      "After standardization (3255, 441)\n",
      "After converting categoricals:\t(3255, 1432)\n",
      "Shapes: bX_train (3255, 1432) - by_train (3255,)\n",
      "\n",
      "Country C\n",
      "Input shape:\t(6469, 163)\n",
      "After standardization (6469, 163)\n",
      "After converting categoricals:\t(6469, 795)\n",
      "Shapes: cX_train (6469, 795) - cy_train (6469,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Country A\")\n",
    "aX_train = pre_process_data(a_train.drop('poor', axis=1))\n",
    "a_train.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "print(\"Shapes: aX_train {} - ay_train {}\".format(aX_train.shape, ay_train.shape))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "bX_train = pre_process_data(b_train.drop('poor', axis=1))\n",
    "b_train.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "print(\"Shapes: bX_train {} - by_train {}\".format(bX_train.shape, by_train.shape))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "cX_train = pre_process_data(c_train.drop('poor', axis=1))\n",
    "cy_train = np.ravel(c_train.poor.astype(int))\n",
    "print(\"Shapes: cX_train {} - cy_train {}\".format(cX_train.shape, cy_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x, y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "def train_model(dtrain, params=None, num_round=100):\n",
    "    if params is None:\n",
    "        params = {'max_depth': 4, 'eta': 100, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dtrain, a_dtest, ay_train, ay_test = prepare_data(aX_train, ay_train)\n",
    "b_dtrain, b_dtest, by_train, by_test = prepare_data(bX_train, by_train)\n",
    "c_dtrain, c_dtest, cy_train, cy_test = prepare_data(cX_train, cy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the optimal hyperparameters\n",
    "def tune_params(dtrain, dtest, y_test):\n",
    "    params = {'max_depth': 4, 'eta': 0.1, 'silent': 1, 'lambda': 1, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "    current_loss = 10000\n",
    "    for num_round in [10, 50, 100]:\n",
    "        for max_depth in range(3, 20):\n",
    "            for eta in [0.01, 0.05, 0.09, 0.1]:\n",
    "                for min_child_weight in [1, 2, 5]:\n",
    "                    for lamda in [0.5, 1]:\n",
    "                        for alpha in [0.5, 1, 2]:\n",
    "                            for lambda_bias in [0.5, 1, 2]:\n",
    "                                params['max_depth'] = max_depth\n",
    "                                params['eta'] = eta\n",
    "                                params['min_child_weight'] = min_child_weight\n",
    "                                params['lambda'] = lamda\n",
    "                                params['lambda_bias'] = lambda_bias\n",
    "                                params['alpha'] = alpha\n",
    "\n",
    "                                model = train_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "                                pred = model.predict(dtest)\n",
    "                                loss = log_loss(pred, y_test)\n",
    "\n",
    "                                if loss < current_loss:\n",
    "                                    current_loss = loss\n",
    "                                    best_hyperparams = params\n",
    "                                    best_num_rounds = num_round\n",
    "\n",
    "    return best_hyperparams, best_num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuning parameters for Country A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c7e6df7bed0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nTuning parameters for Country A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_num_rounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_dtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_dtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0may_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(\"\\n\\nTuning parameters for Country B\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#b_params, b_num_rounds = tune_params(b_dtrain, b_dtest, by_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a94fe02070f7>\u001b[0m in \u001b[0;36mtune_params\u001b[0;34m(dtrain, dtest, y_test)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f09cca13ec34>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dtrain, params, num_round)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eta'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'silent'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'objective'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'reg:logistic'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/poverty/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "print(\"\\n\\nTuning parameters for Country A\")\n",
    "a_params, a_num_rounds = tune_params(a_dtrain, a_dtest, ay_test)\n",
    "#print(\"\\n\\nTuning parameters for Country B\")\n",
    "#b_params, b_num_rounds = tune_params(b_dtrain, b_dtest, by_test)\n",
    "#print(\"\\n\\nTuning parameters for Country C\")\n",
    "#c_params, c_num_rounds = tune_params(c_dtrain, c_dtest, cy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A params: {'max_depth': 9, 'eta': 0.1, 'silent': 1, 'lambda': 1, 'min_child_weight': 5, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
      "B params: {'max_depth': 9, 'eta': 0.1, 'silent': 1, 'lambda': 1, 'min_child_weight': 5, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
      "C params: {'max_depth': 9, 'eta': 0.1, 'silent': 1, 'lambda': 1, 'min_child_weight': 5, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "print(\"A params: {}\".format(a_params))\n",
    "#print(\"B params: {}\".format(b_params))\n",
    "#print(\"C params: {}\".format(c_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_params = {'max_depth': 5, 'eta': 0.05, 'silent': 1, 'lambda': 2, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "a_params['silent'] = 0\n",
    "num_round = 3000\n",
    "\n",
    "model_a = train_model(a_dtrain, params=a_params, num_round=num_round)\n",
    "\n",
    "a_pred = model_a.predict(a_dtest)\n",
    "a_pred_train = model_a.predict(a_dtrain)\n",
    "\n",
    "test_loss_a = log_loss(a_pred, ay_test)\n",
    "train_loss_a = log_loss(a_pred_train, ay_train)\n",
    "\n",
    "print(\"Loss A Test: {} - Train: {}\".format(test_loss_a, train_loss_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_params = a_params\n",
    "b_params['silent'] = 0\n",
    "num_round = 3000\n",
    "\n",
    "model_b = train_model(b_dtrain, params=b_params, num_round=num_round)\n",
    "\n",
    "b_pred = model_b.predict(b_dtest)\n",
    "b_pred_train = model_b.predict(b_dtrain)\n",
    "\n",
    "test_loss_b = log_loss(b_pred, by_test)\n",
    "train_loss_b = log_loss(b_pred_train, by_train)\n",
    "\n",
    "print(\"Loss B Test: {} - Train: {}\".format(test_loss_b, train_loss_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_params = a_params\n",
    "c_params['silent'] = 0\n",
    "\n",
    "model_c = train_model(c_dtrain, params=c_params, num_round=3000)\n",
    "\n",
    "c_pred = model_c.predict(c_dtest)\n",
    "c_pred_train = model_c.predict(c_dtrain)\n",
    "\n",
    "test_loss_c = log_loss(c_pred, cy_test)\n",
    "train_loss_c = log_loss(c_pred_train, cy_train)\n",
    "\n",
    "print(\"Loss C Test: {} - Train: {}\".format(test_loss_c, train_loss_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg Train loss: {}\".format(np.mean([train_loss_a, train_loss_b, train_loss_c])))\n",
    "print(\"Avg Test loss: {}\".format(np.mean([test_loss_a, test_loss_b, test_loss_c])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 344)\n",
      "After standardization (4041, 344)\n",
      "After converting categoricals:\t(4041, 851)\n",
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n",
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n"
     ]
    }
   ],
   "source": [
    "# process the test data\n",
    "a_test = pre_process_data(a_test, enforce_cols=aX_train.columns)\n",
    "b_test = pre_process_data(b_test, enforce_cols=bX_train.columns)\n",
    "c_test = pre_process_data(c_test, enforce_cols=cX_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_preds = model_a.predict(a_test)\n",
    "b_preds = model_b.predict(b_test)\n",
    "c_preds = model_c.predict(c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds[:, 1],  # proba p=1\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert preds to data frames\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([a_sub, b_sub, c_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
