{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "def balance(df):\n",
    "    poor = df[df['poor'] == True]\n",
    "    not_poor = df[df['poor'] == False]\n",
    "    poor_upsampled = resample(poor, \n",
    "                              replace=True,\n",
    "                              n_samples=len(not_poor),\n",
    "                              random_state=42)\n",
    "    return pd.concat([poor_upsampled, not_poor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Input shape:\t(8203, 344)\n",
      "After standardization (8203, 344)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2754: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After converting categoricals:\t(8203, 859)\n",
      "\n",
      "Country B\n",
      "Input shape:\t(6008, 441)\n",
      "After standardization (6008, 441)\n",
      "After converting categoricals:\t(6008, 1432)\n",
      "\n",
      "Country C\n",
      "Input shape:\t(10992, 163)\n",
      "After standardization (10992, 163)\n",
      "After converting categoricals:\t(10992, 795)\n"
     ]
    }
   ],
   "source": [
    "print(\"Country A\")\n",
    "a_numeric = a_train.select_dtypes(include=['int64', 'float64']) #Tested Random forests and XGBoost with only numeric values. Did not work very well\n",
    "a_numeric.fillna(0, inplace=True)\n",
    "\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "aX_train = pre_process_data(a_train.drop('poor', axis=1))\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "b_train = balance(b_train)\n",
    "bX_train = pre_process_data(b_train.drop('poor', axis=1))\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "c_train = balance(c_train)\n",
    "cX_train = pre_process_data(c_train.drop('poor', axis=1))\n",
    "cy_train = np.ravel(c_train.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def prepare_data(x, y, test_size=0.2, xgb_format=True):\n",
    "    if test_size == 0:\n",
    "        dtrain = x\n",
    "        Y_train = y\n",
    "        dtest = None\n",
    "        Y_test = None\n",
    "    else:\n",
    "        dtrain, dtest, Y_train, Y_test = train_test_split(x, y, test_size=test_size, stratify=y, random_state=42)\n",
    "\n",
    "    if xgb_format:\n",
    "        dtrain = xgb.DMatrix(dtrain, label=Y_train)\n",
    "        if dtest is not None:\n",
    "            dtest = xgb.DMatrix(dtest)\n",
    "\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "def train_rf_model(features, labels, **kwargs):\n",
    "\n",
    "    # instantiate model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "    # train model\n",
    "    model.fit(features, labels)\n",
    "\n",
    "    # get a (not-very-useful) sense of performance\n",
    "    accuracy = model.score(features, labels)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_xgb_model(dtrain, params=None, num_round=100):\n",
    "    if params is None:\n",
    "        params = {'max_depth': 4, 'eta': 100, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    return bst\n",
    "\n",
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Cross Validate\n",
    "def cross_validate(x_train, x_test, y_train, y_test, model):\n",
    "    test_loss = None\n",
    "    if x_test is not None:\n",
    "        preds = model.predict(x_test)\n",
    "        test_loss = log_loss(preds, y_test)\n",
    "\n",
    "    preds_train = model.predict(x_train)\n",
    "    train_loss = log_loss(preds_train, y_train)\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "rf_ax_train, rf_ax_test, rf_ay_train, rf_ay_test = prepare_data(aX_train, ay_train, test_size=test_size, xgb_format=False)\n",
    "rf_bx_train, rf_bx_test, rf_by_train, rf_by_test = prepare_data(bX_train, by_train, test_size=test_size, xgb_format=False)\n",
    "rf_cx_train, rf_cx_test, rf_cy_train, rf_cy_test = prepare_data(cX_train, cy_train, test_size=test_size, xgb_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 77.92%\n",
      "In-sample accuracy: 100.00%\n",
      "In-sample accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "rf_a = train_rf_model(rf_ax_train, rf_ay_train)\n",
    "rf_b = train_rf_model(rf_bx_train, rf_by_train)\n",
    "rf_c = train_rf_model(rf_cx_train, rf_cy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 7.626831012674126 - Test: 8.629536036664236\n",
      "B Loss. Train: 9.992007221626413e-16 - Test: 2.4936394347812465\n",
      "C Loss. Train: 9.992007221626413e-16 - Test: 0.32030395109214166\n"
     ]
    }
   ],
   "source": [
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(rf_ax_train, rf_ax_test, rf_ay_train, rf_ay_test, rf_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(rf_bx_train, rf_bx_test, rf_by_train, rf_by_test, rf_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(rf_cx_train, rf_cx_test, rf_cy_train, rf_cy_test, rf_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds,\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]\n",
    "\n",
    "\n",
    "def prepare_submission(data_paths, models, enforce_cols=None, to_keep_cols=None, xgb_format=False):\n",
    "    # load test data\n",
    "    a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "    b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "    c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "    if to_keep_cols:\n",
    "        a_test = a_test[to_keep_cols['a']]\n",
    "        b_test = b_test[to_keep_cols['b']]\n",
    "        c_test = c_test[to_keep_cols['c']]\n",
    "\n",
    "    if enforce_cols:\n",
    "        # process the test data\n",
    "        a_test = pre_process_data(a_test, enforce_cols=enforce_cols['a'])\n",
    "        b_test = pre_process_data(b_test, enforce_cols=enforce_cols['b'])\n",
    "        c_test = pre_process_data(c_test, enforce_cols=enforce_cols['c'])\n",
    "\n",
    "    a_test.fillna(0, inplace=True)\n",
    "    b_test.fillna(0, inplace=True)\n",
    "    c_test.fillna(0, inplace=True)\n",
    "\n",
    "    if xgb_format:\n",
    "        a_test = xgb.DMatrix(a_test)\n",
    "        b_test = xgb.DMatrix(b_test)\n",
    "        c_test = xgb.DMatrix(c_test)\n",
    "\n",
    "    # TODO: use probabilities\n",
    "    a_preds = models['a'].predict(a_test)\n",
    "    b_preds = models['b'].predict(b_test)\n",
    "    c_preds = models['c'].predict(c_test)\n",
    "    \n",
    "    a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "    b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "    c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "    \n",
    "    submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 344)\n",
      "After standardization (4041, 344)\n",
      "After converting categoricals:\t(4041, 851)\n",
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n",
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n"
     ]
    }
   ],
   "source": [
    "# Prepare submission\n",
    "models = {'a': rf_a, 'b': rf_b, 'c': rf_c}\n",
    "enforce_cols = {'a': aX_train.columns, 'b': bX_train.columns, 'c': cX_train.columns}\n",
    "\n",
    "submission = prepare_submission(data_paths, models, enforce_cols)\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aX_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9e520afde90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxgb_ax_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_ax_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_ay_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_ay_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0may_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mxgb_bx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_bx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_by_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_by_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_cx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_cx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_cy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_cy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aX_train' is not defined"
     ]
    }
   ],
   "source": [
    "test_size = 0.\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train, ay_train, test_size=test_size, xgb_format=True)\n",
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train, by_train, test_size=test_size, xgb_format=True)\n",
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 3000\n",
    "params = {'max_depth': 2, 'eta': 0.05, 'silent': 0, 'lambda': 2, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)\n",
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)\n",
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 11.078467278889883 - Test: None\n",
      "B Loss. Train: 3.2234598696340795 - Test: None\n",
      "C Loss. Train: 0.14421621963818088 - Test: None\n",
      "Total loss: 14.301927148523962\n"
     ]
    }
   ],
   "source": [
    "a_train_loss, a_test_loss = cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)\n",
    "b_train_loss, b_test_loss = cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)\n",
    "c_train_loss, c_test_loss = cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)\n",
    "\n",
    "print(\"A Loss. Train: {} - Test: {}\".format(a_train_loss, a_test_loss))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(b_train_loss, b_test_loss))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(c_train_loss, c_test_loss))\n",
    "print(\"Total loss: {}\".format(sum([a_train_loss, b_train_loss])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with columns with more entropy (Submitted Jan 8 2018. Score:  0.24977 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(a):\n",
    "    return - sum( (a / sum(a)) * np.log((a / sum(a))))\n",
    "\n",
    "\n",
    "def get_entropies(df):\n",
    "    entropies = []\n",
    "    for col in df.columns.tolist():\n",
    "        res = df[col].value_counts()\n",
    "        entropies.append(entropy(res.values))\n",
    "\n",
    "    return entropies\n",
    "\n",
    "\n",
    "def get_low_entropy_columns(df):\n",
    "    to_del = []\n",
    "    entropies = get_entropies(df)\n",
    "    median_entr = np.median(entropies)\n",
    "    #std_entr = np.std(entropies)\n",
    "    #avg_entr = np.mean(entropies)\n",
    "    for i, col in enumerate(df.columns.tolist()):\n",
    "        if entropies[i] < median_entr:\n",
    "            to_del.append(col)\n",
    "    return to_del\n",
    "\n",
    "\n",
    "def filter_columns(df):\n",
    "    to_del = get_low_entropy_columns(df)\n",
    "    print(\"Total columns: {}. To delete: {}\".format(len(df.columns.tolist()), len(to_del)))\n",
    "    to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "    return df[list(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Input shape:\t(8203, 172)\n",
      "After standardization (8203, 172)\n",
      "After converting categoricals:\t(8203, 504)\n",
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Input shape:\t(6008, 221)\n",
      "After standardization (6008, 221)\n",
      "After converting categoricals:\t(6008, 934)\n",
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Input shape:\t(10992, 82)\n",
      "After standardization (10992, 82)\n",
      "After converting categoricals:\t(10992, 562)\n"
     ]
    }
   ],
   "source": [
    "# We need to repreprocess the data with less columns\n",
    "print(\"Country A\")\n",
    "a_train_reduc = filter_columns(a_train.drop('poor', axis=1))\n",
    "aX_train = pre_process_data(a_train_reduc)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train_reduc = filter_columns(b_train.drop('poor', axis=1))\n",
    "bX_train = pre_process_data(b_train_reduc)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train_reduc = filter_columns(c_train.drop('poor', axis=1))\n",
    "cX_train = pre_process_data(c_train_reduc)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train, ay_train, test_size=test_size, xgb_format=True)\n",
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train, by_train, test_size=test_size, xgb_format=True)\n",
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 8000\n",
    "params = {'max_depth': 5, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)\n",
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)\n",
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 0.4295274568537806 - Test: 5.117176587617911\n",
      "B Loss. Train: 0.20367149268877077 - Test: 0.9906044571416714\n",
      "C Loss. Train: 0.030888632103723165 - Test: 0.09568105622696856\n"
     ]
    }
   ],
   "source": [
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n",
      "Input shape:\t(1604, 221)\n",
      "After standardization (1604, 221)\n",
      "After converting categoricals:\t(1604, 925)\n",
      "Input shape:\t(3187, 82)\n",
      "After standardization (3187, 82)\n",
      "After converting categoricals:\t(3187, 547)\n",
      "(4041, 498)\n",
      "(1604, 936)\n",
      "(3187, 564)\n"
     ]
    }
   ],
   "source": [
    "# Prepare submission\n",
    "models = {'a': xgb_a, 'b': xgb_b, 'c': xgb_c}\n",
    "a_keep = a_train_reduc.columns.tolist()\n",
    "b_keep = b_train_reduc.columns.tolist()\n",
    "c_keep = c_train_reduc.columns.tolist()\n",
    "\n",
    "#to_keep_cols = {'a': a_keep, 'b': b_keep, 'c': c_keep}\n",
    "#enforce_cols = {'a': a_keep, 'b': b_keep, 'c': c_keep}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(a_test.shape)\n",
    "print(b_test.shape)\n",
    "print(c_test.shape)\n",
    "\n",
    "a_testxgb = xgb.DMatrix(a_test)\n",
    "b_testxgb = xgb.DMatrix(b_test)\n",
    "c_testxgb = xgb.DMatrix(c_test)\n",
    "\n",
    "# TODO: use probabilities\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "\n",
    "#submission = prepare_submission(data_paths, models, enforce_cols=enforce_cols, to_keep_cols=to_keep_cols, xgb_format=True)\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "a_test = a_test[to_keep_cols['a']]\n",
    "a_test = pre_process_data(a_test)\n",
    "'yeHQSlwg_QjxqO' in a_train_reduc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests with columns with more entropy (Submitted Jan 8. Score:  3.93945 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Input shape:\t(1855, 172)\n",
      "After standardization (1855, 172)\n",
      "After converting categoricals:\t(1855, 498)\n",
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Input shape:\t(3255, 221)\n",
      "After standardization (3255, 221)\n",
      "After converting categoricals:\t(3255, 936)\n",
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Input shape:\t(6469, 82)\n",
      "After standardization (6469, 82)\n",
      "After converting categoricals:\t(6469, 564)\n"
     ]
    }
   ],
   "source": [
    "# We need to repreprocess the data with less columns\n",
    "print(\"Country A\")\n",
    "a_train_reduc = filter_columns(a_train.drop('poor', axis=1))\n",
    "aX_train = pre_process_data(a_train_reduc)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "b_train_reduc = filter_columns(b_train.drop('poor', axis=1))\n",
    "bX_train = pre_process_data(b_train_reduc)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "c_train_reduc = filter_columns(c_train.drop('poor', axis=1))\n",
    "cX_train = pre_process_data(c_train_reduc)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.\n",
    "\n",
    "rf_ax_train, rf_ax_test, rf_ay_train, rf_ay_test = prepare_data(aX_train, ay_train, test_size=test_size, xgb_format=False)\n",
    "rf_bx_train, rf_bx_test, rf_by_train, rf_by_test = prepare_data(bX_train, by_train, test_size=test_size, xgb_format=False)\n",
    "rf_cx_train, rf_cx_test, rf_cy_train, rf_cy_test = prepare_data(cX_train, cy_train, test_size=test_size, xgb_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy: 100.00%\n",
      "In-sample accuracy: 99.97%\n",
      "In-sample accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "rf_a = train_rf_model(rf_ax_train, rf_ay_train)\n",
    "rf_b = train_rf_model(rf_bx_train, rf_by_train)\n",
    "rf_c = train_rf_model(rf_cx_train, rf_cy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 9.992007221626413e-16 - Test: None\n",
      "B Loss. Train: 0.010611236864007414 - Test: None\n",
      "C Loss. Train: 9.992007221626413e-16 - Test: None\n"
     ]
    }
   ],
   "source": [
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(rf_ax_train, rf_ax_test, rf_ay_train, rf_ay_test, rf_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(rf_bx_train, rf_bx_test, rf_by_train, rf_by_test, rf_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(rf_cx_train, rf_cx_test, rf_cy_train, rf_cy_test, rf_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n",
      "Input shape:\t(1604, 221)\n",
      "After standardization (1604, 221)\n",
      "After converting categoricals:\t(1604, 925)\n",
      "Input shape:\t(3187, 82)\n",
      "After standardization (3187, 82)\n",
      "After converting categoricals:\t(3187, 547)\n",
      "(4041, 498)\n",
      "(1604, 936)\n",
      "(3187, 564)\n"
     ]
    }
   ],
   "source": [
    "# Prepare submission\n",
    "a_keep = a_train_reduc.columns.tolist()\n",
    "b_keep = b_train_reduc.columns.tolist()\n",
    "c_keep = c_train_reduc.columns.tolist()\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(a_test.shape)\n",
    "print(b_test.shape)\n",
    "print(c_test.shape)\n",
    "\n",
    "#a_testxgb = xgb.DMatrix(a_test)\n",
    "#b_testxgb = xgb.DMatrix(b_test)\n",
    "#c_testxgb = xgb.DMatrix(c_test)\n",
    "\n",
    "# TODO: use probabilities\n",
    "a_preds = rf_a.predict(a_test)\n",
    "b_preds = rf_b.predict(b_test)\n",
    "c_preds = rf_c.predict(c_test)\n",
    "\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "\n",
    "#submission = prepare_submission(data_paths, models, enforce_cols=enforce_cols, to_keep_cols=to_keep_cols, xgb_format=True)\n",
    "submission.to_csv('submission_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
