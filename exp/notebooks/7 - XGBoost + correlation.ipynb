{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import scipy.stats as ss\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(DATA_PATHS['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(DATA_PATHS['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(DATA_PATHS['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets filter out the columns with high correlation\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "def get_highly_correlated_columns(df):\n",
    "    df_objs = df.select_dtypes(include=['O'])\n",
    "    del df_objs['country']\n",
    "    corr_matrix = pd.DataFrame(columns=df_objs.columns, index=df_objs.columns.tolist())\n",
    "    with tqdm(total=len(corr_matrix.columns.tolist()) * len(corr_matrix.columns.tolist())) as pbar:\n",
    "        for col1 in df_objs.columns.tolist():\n",
    "            for col2 in df_objs.columns.tolist():\n",
    "                if col1 != col2:\n",
    "                    confusion_matrix = pd.crosstab(df_objs[col1], df_objs[col2])\n",
    "                    corr = cramers_corrected_stat(confusion_matrix)\n",
    "                else:\n",
    "                    corr = 1\n",
    "                corr_matrix.loc[col1, col2] = corr\n",
    "                pbar.update(1)\n",
    "    \n",
    "    cols = {}\n",
    "    for c1 in corr_matrix.columns.tolist():\n",
    "        s = corr_matrix.loc[c1]\n",
    "        s = s[s > 0.5]\n",
    "        s = list(s.index)\n",
    "        s.remove(c1)\n",
    "        cols[c1] = s\n",
    "\n",
    "    cols = {k: cols[k] for k in cols if cols[k]}\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1095/114921 [00:05<09:58, 190.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-292-eac66b678095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma_corr_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_highly_correlated_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb_corr_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_highly_correlated_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc_corr_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_highly_correlated_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-291-a64bb3cae7b7>\u001b[0m in \u001b[0;36mget_highly_correlated_columns\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_objs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcol2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcramers_corrected_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mcrosstab\u001b[0;34m(index, columns, values, rownames, colnames, aggfunc, margins, dropna, normalize)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__dummy__'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         table = df.pivot_table('__dummy__', index=rownames, columns=colnames,\n\u001b[0;32m--> 493\u001b[0;31m                                aggfunc=len, margins=margins, dropna=dropna)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_index\u001b[0;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by)\u001b[0m\n\u001b[1;32m   3241\u001b[0m             \u001b[0;31m# make sure that the axis is lexsorted to start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m             \u001b[0;31m# if not we need to reconstruct to get the correct indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_levels_monotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3244\u001b[0m             indexer = lexsort_indexer(labels._get_labels_for_sorting(),\n\u001b[1;32m   3245\u001b[0m                                       \u001b[0morders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_sort_levels_monotonic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_lexsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.cache_readonly.__get__ (pandas/_libs/lib.c:44594)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mis_monotonic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m         increasing) values.\n\u001b[1;32m    689\u001b[0m         \"\"\"\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_increasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.cache_readonly.__get__ (pandas/_libs/lib.c:44594)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mis_monotonic_increasing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0msort_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRangeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/poverty/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a_corr_cols = get_highly_correlated_columns(a_train)\n",
    "b_corr_cols = get_highly_correlated_columns(b_train)\n",
    "c_corr_cols = get_highly_correlated_columns(c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed from A: 103\n",
      "Columns removed from B: 183\n",
      "Columns removed from C: 75\n"
     ]
    }
   ],
   "source": [
    "# Lets delete the highly correlated columns in a greedy manner\n",
    "def delete_columns(df, dict_cols):\n",
    "    to_delete = list(set([elem for values in dict_cols.values() for elem in values]))\n",
    "    return df.drop(to_delete, axis=1)\n",
    "\n",
    "\n",
    "a_train_small = delete_columns(a_train, a_corr_cols)\n",
    "b_train_small = delete_columns(b_train, b_corr_cols)\n",
    "c_train_small = delete_columns(c_train, c_corr_cols)\n",
    "\n",
    "\n",
    "print(\"Columns removed from A: {}\".format(len(a_train.columns) - len(a_train_small.columns)))\n",
    "print(\"Columns removed from B: {}\".format(len(b_train.columns) - len(b_train_small.columns)))\n",
    "print(\"Columns removed from C: {}\".format(len(c_train.columns) - len(c_train_small.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we deleted hundreds of columns just by checking their correlation. Now lets train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "def balance(df):\n",
    "    poor = df[df['poor'] == True]\n",
    "    not_poor = df[df['poor'] == False]\n",
    "    poor_upsampled = resample(poor, \n",
    "                              replace=True,\n",
    "                              n_samples=len(not_poor),\n",
    "                              random_state=42)\n",
    "    return pd.concat([poor_upsampled, not_poor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Input shape:\t(9000, 172)\n",
      "After standardization (9000, 172)\n",
      "After converting categoricals:\t(9000, 504)\n"
     ]
    }
   ],
   "source": [
    "# Filter out columns with low entropy\n",
    "print(\"Country A\")\n",
    "#a_train_reduc = a_train_small\n",
    "a_train_bala = balance(a_train)\n",
    "a_train_reduc = filter_columns(a_train_bala.drop('poor', axis=1))\n",
    "aX_train = pre_process_data(a_train_reduc)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train_bala.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Input shape:\t(6008, 221)\n",
      "After standardization (6008, 221)\n",
      "After converting categoricals:\t(6008, 934)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry B\")\n",
    "#b_train_reduc = b_train_small\n",
    "b_train_bala = balance(b_train)\n",
    "b_train_reduc = filter_columns(b_train_bala.drop('poor', axis=1))\n",
    "bX_train = pre_process_data(b_train_reduc)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train_bala.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Input shape:\t(10992, 82)\n",
      "After standardization (10992, 82)\n",
      "After converting categoricals:\t(10992, 562)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry C\")\n",
    "#c_train_reduc = c_train_small\n",
    "c_train_bala = balance(c_train)\n",
    "c_train_reduc = filter_columns(c_train_bala.drop('poor', axis=1))\n",
    "cX_train = pre_process_data(c_train_reduc)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train_bala.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit this step if you dont want to do dimension reduction\n",
    "from sklearn.decomposition import TruncatedSVD  # PCA\n",
    "\n",
    "def reduce_dimensions(x, n_comp=90):\n",
    "    #pca = PCA(n_components=40)\n",
    "    svd = TruncatedSVD(n_components=n_comp, n_iter=7, random_state=42)\n",
    "    return svd.fit_transform(x), svd\n",
    "\n",
    "# reduce dimensions for all countries\n",
    "aX_train_svd, a_svd = reduce_dimensions(aX_train)\n",
    "bX_train_svd, b_svd = reduce_dimensions(bX_train)\n",
    "cX_train_svd, c_svd = reduce_dimensions(cX_train)\n",
    "\n",
    "#aX_train = aX_train_svd\n",
    "#bX_train = bX_train_svd\n",
    "#cX_train = cX_train_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to train\n",
    "test_size = 0.2\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train_svd, ay_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train_svd, by_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train_svd, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 8000\n",
    "#params = {'max_depth': 15, 'eta': 0.01, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "params = {'max_depth': 15, 'eta': 0.01, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 0.15369252708486553 - Test: 6.0330821042947065\n"
     ]
    }
   ],
   "source": [
    "# With PCA reduction (40 dims): 0.7444784675249769\n",
    "# Without reduction: 0.7105216116701498\n",
    "# With SVD reduction: 0.585940340593645\n",
    "# With Cramers + SVD: 0.22948565367793244\n",
    "# With Entropy + SVD: 0.1343193065703149\n",
    "\n",
    "#xgb_ax_test = xgb_ax_train\n",
    "#xgb_ay_test = xgb_ay_train\n",
    "a_loss_train, a_loss_test = cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)\n",
    "\n",
    "print(\"A Loss. Train: {} - Test: {}\".format(a_loss_train, a_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Loss. Train: 0.1946223943355792 - Test: 2.649129660741833\n"
     ]
    }
   ],
   "source": [
    "#xgb_bx_test = xgb_bx_train\n",
    "#xgb_by_test = xgb_by_train\n",
    "b_loss_train, b_loss_test = cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(b_loss_train, b_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Loss. Train: 0.16967765326534057 - Test: 4.680354694093998\n"
     ]
    }
   ],
   "source": [
    "#xgb_cx_test = xgb_cx_train\n",
    "#xgb_cy_test = xgb_cy_train\n",
    "c_loss_train, c_loss_test = cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(c_loss_train, c_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Train loss: 0.16689242105276678\n"
     ]
    }
   ],
   "source": [
    "# Avg loss:\n",
    "lines = sum([len(aX_train), len(bX_train), len(cX_train)])\n",
    "total_loss = np.average([a_loss_train, b_loss_train, c_loss_train], weights=[len(aX_train) / lines, len(bX_train) / lines, len(cX_train) / lines])\n",
    "print(\"Averaged Train loss: {}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Test loss: 4.930524017590127\n"
     ]
    }
   ],
   "source": [
    "# Avg loss:\n",
    "lines = sum([len(aX_train), len(bX_train), len(cX_train)])  # It doesn't matter if we use train here, since the proportions will maintain\n",
    "total_loss = np.average([a_loss_test, b_loss_test, c_loss_test], weights=[len(aX_train) / lines, len(bX_train) / lines, len(cX_train) / lines])\n",
    "print(\"Averaged Test loss: {}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 172)\n",
      "After standardization (4041, 172)\n",
      "After converting categoricals:\t(4041, 500)\n",
      "Input shape:\t(1604, 221)\n",
      "After standardization (1604, 221)\n",
      "After converting categoricals:\t(1604, 923)\n",
      "Input shape:\t(3187, 82)\n",
      "After standardization (3187, 82)\n",
      "After converting categoricals:\t(3187, 546)\n",
      "Submission saved.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Submission\n",
    "# TODO: tidy this up\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(DATA_PATHS['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(DATA_PATHS['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(DATA_PATHS['C']['test'], index_col='id')\n",
    "\n",
    "# columns to keep from test data\n",
    "a_keep = a_train_reduc.columns.tolist()\n",
    "b_keep = b_train_reduc.columns.tolist()\n",
    "c_keep = c_train_reduc.columns.tolist()\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "# Create dummies, standarize numeric values\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "# Reduce dimensions (comment if not testing this)\n",
    "a_test_svd = a_svd.transform(a_test)\n",
    "b_test_svd = b_svd.transform(b_test)\n",
    "c_test_svd = c_svd.transform(c_test)\n",
    "\n",
    "# Create XGBoost matrix\n",
    "a_testxgb = xgb.DMatrix(a_test_svd)\n",
    "b_testxgb = xgb.DMatrix(b_test_svd)\n",
    "c_testxgb = xgb.DMatrix(c_test_svd)\n",
    "\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "# Prepare dataframes for each country\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "submission.to_csv('submission_xgb_entropy_svd_balancing_BESTSOFAR.csv')\n",
    "print(\"Submission saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 344)\n",
      "After standardization (4041, 344)\n",
      "After converting categoricals:\t(4041, 851)\n",
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n",
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n",
      "Submission saved.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Submission\n",
    "# TODO: tidy this up\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(DATA_PATHS['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(DATA_PATHS['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(DATA_PATHS['C']['test'], index_col='id')\n",
    "\n",
    "\n",
    "\n",
    "a_test_small = delete_columns(a_test, a_corr_cols)\n",
    "b_test_small = delete_columns(b_test, b_corr_cols)\n",
    "c_test_small = delete_columns(c_test, c_corr_cols)\n",
    "\n",
    "# Create dummies, standarize numeric values\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Reduce dimensions (comment if not testing this)\n",
    "a_test_svd = a_svd.transform(a_test)\n",
    "b_test_svd = b_svd.transform(b_test)\n",
    "c_test_svd = c_svd.transform(c_test)\n",
    "\n",
    "\n",
    "# Create XGBoost matrix\n",
    "a_testxgb = xgb.DMatrix(a_test_svd)\n",
    "b_testxgb = xgb.DMatrix(b_test_svd)\n",
    "c_testxgb = xgb.DMatrix(c_test_svd)\n",
    "\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "# Prepare dataframes for each country\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "submission.to_csv('submission_xgb_cramers_svd.csv')\n",
    "print(\"Submission saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the optimal hyperparameters for each country\n",
    "def tune_params(dtrain, dtest, y_test):\n",
    "    params = {\n",
    "        'max_depth': 20,\n",
    "        'eta': 0.05, \n",
    "        'silent': 0,\n",
    "        'lambda': 0.5,\n",
    "        'alpha': 0.5,\n",
    "        'lambda_bias': 0.5, \n",
    "        'min_child_weight': 1,\n",
    "        'objective': 'binary:logistic', \n",
    "        'eval_metric': 'logloss', \n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    current_loss = 10000\n",
    "    best_num_rounds = 0\n",
    "    best_hyperparams = {}\n",
    "    num_rounds = [2000]\n",
    "    max_depths = [2, 3, 5, 10]\n",
    "    etas = [0.01, 0.05]\n",
    "    min_child_weights = [1, 2]\n",
    "    lamdas = [0.5, 1]\n",
    "    alphas = [0.5, 1]\n",
    "    lambda_biases = [0.5, 1]\n",
    "    total_combinations = len(num_rounds) * len(max_depths) * len(etas)*\\\n",
    "        len(min_child_weights) * len(lamdas) * len(alphas) * len(lambda_biases)\n",
    "\n",
    "    with tqdm(total=total_combinations) as pbar:\n",
    "        for num_round in num_rounds:\n",
    "            for max_depth in max_depths:\n",
    "                for eta in etas:\n",
    "                    for min_child_weight in min_child_weights:\n",
    "                        for lamda in lamdas:\n",
    "                            for alpha in alphas:\n",
    "                                for lambda_bias in lambda_biases:\n",
    "                                    params['max_depth'] = max_depth\n",
    "                                    params['eta'] = eta\n",
    "                                    params['min_child_weight'] = min_child_weight\n",
    "                                    params['lambda'] = lamda\n",
    "                                    params['alpha'] = alpha\n",
    "                                    params['lambda_bias'] = lambda_bias\n",
    "\n",
    "                                    model = train_xgb_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "                                    pred = model.predict(dtest)\n",
    "                                    loss = log_loss(pred, y_test)\n",
    "\n",
    "                                    if loss < current_loss:\n",
    "                                        current_loss = loss\n",
    "                                        best_hyperparams = params\n",
    "                                        best_num_rounds = num_round\n",
    "                                    pbar.update(1)\n",
    "\n",
    "    return best_hyperparams, best_num_rounds, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [34:39<00:00, 15.54s/it]\n",
      "100%|██████████| 128/128 [59:45<00:00, 29.36s/it]\n",
      "100%|██████████| 128/128 [1:49:13<00:00, 53.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A best params for loss: 3.576437807163302 :\n",
      "{'max_depth': 10, 'eta': 0.05, 'silent': 0, 'lambda': 1, 'alpha': 1, 'lambda_bias': 1, 'min_child_weight': 2, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "a_best_params = {}\n",
    "a_best_rounds = 0\n",
    "a_best_loss = 1000\n",
    "a_best_svd = None\n",
    "for n in [50, 100, 200]:\n",
    "    aX_train_svd, a_svd = reduce_dimensions(aX_train, n_comp=n)\n",
    "    xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train_svd, ay_train, test_size=test_size, xgb_format=True)\n",
    "    a_params, a_num_rounds, loss = tune_params(xgb_ax_train, xgb_ax_test, xgb_ay_test)\n",
    "    if loss < a_best_loss:\n",
    "        a_best_params = a_params\n",
    "        a_best_rounds = a_num_rounds\n",
    "        a_best_loss = loss\n",
    "        a_best_svd = a_svd\n",
    "\n",
    "print(\"A best params for loss: {} :\".format(a_best_loss))\n",
    "print(a_best_params)\n",
    "print(a_best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "b_best_params = {}\n",
    "b_best_rounds = 0\n",
    "b_best_loss = 1000\n",
    "b_best_svd = None\n",
    "for n in [50, 100, 200]:\n",
    "    bX_train_svd, b_svd = reduce_dimensions(bX_train, n_comp=n)\n",
    "    xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train_svd, by_train, test_size=test_size, xgb_format=True)\n",
    "    b_params, b_num_rounds, loss = tune_params(xgb_bx_train, xgb_bx_test, xgb_by_test)\n",
    "    if loss < b_best_loss:\n",
    "        b_best_params = b_params\n",
    "        b_best_rounds = b_num_rounds\n",
    "        b_best_loss = loss\n",
    "        b_best_svd = b_svd\n",
    "\n",
    "print(\"B best params for loss: {} :\".format(b_best_loss))\n",
    "print(b_best_params)\n",
    "print(b_best_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "c_best_params = {}\n",
    "c_best_rounds = 0\n",
    "c_best_loss = 1000\n",
    "c_best_svd = None\n",
    "for n in [50, 100, 200]:\n",
    "    cX_train_svd, c_svd = reduce_dimensions(cX_train, n_comp=n)\n",
    "    xgc_cx_train, xgc_cx_test, xgc_cy_train, xgc_cy_test = prepare_data(cX_train_svd, cy_train, test_size=test_size, xgc_format=True)\n",
    "    c_params, c_num_rounds, loss = tune_params(xgc_cx_train, xgc_cx_test, xgc_cy_test)\n",
    "    if loss < c_best_loss:\n",
    "        c_best_params = c_params\n",
    "        c_best_rounds = c_num_rounds\n",
    "        c_best_loss = loss\n",
    "        c_best_svd = c_svd\n",
    "\n",
    "print(\"C best params for loss: {} :\".format(c_best_loss))\n",
    "print(c_best_params)\n",
    "print(c_best_rounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
