{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from tqdm import tqdm\n",
    "import scipy as sc\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('../..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    #print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    #print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    #print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "from sklearn.utils import resample\n",
    "def balance(df):\n",
    "    poor = df[df['poor'] == True]\n",
    "    not_poor = df[df['poor'] == False]\n",
    "    \n",
    "    not_poor_downsampled = resample(not_poor, \n",
    "                              replace=True,\n",
    "                              n_samples=int(len(not_poor) * 0.6),\n",
    "                              random_state=42)\n",
    "    \n",
    "    poor_upsampled = resample(poor, \n",
    "                              replace=True,\n",
    "                              n_samples=len(not_poor_downsampled),\n",
    "                              random_state=42)\n",
    "    res = pd.concat([poor_upsampled, not_poor_downsampled])\n",
    "    return res.sample(frac=1)\n",
    "\n",
    "import random\n",
    "\n",
    "def get_best_columns(df, params={}, num_round=500):\n",
    "    test_size = 0.2\n",
    "\n",
    "    X_train = balance(df)\n",
    "    X_train.poor.fillna(False, inplace=True)\n",
    "    y_train = np.ravel(X_train.poor.astype(int))\n",
    "\n",
    "    columns = X_train.columns.tolist()\n",
    "    columns.remove('poor')\n",
    "    columns.remove('country')\n",
    "    good_cols = columns.copy()\n",
    "    \n",
    "    # Initial train\n",
    "    X_train_proc = pre_process_data(pd.DataFrame(X_train[good_cols]))\n",
    "    xgb_x_train, xgb_x_test, xgb_y_train, xgb_y_test = prepare_data(X_train_proc, y_train, test_size=test_size, xgb_format=True)\n",
    "    xgb_model = train_xgb_model(xgb_x_train, params=params, num_round=num_round)\n",
    "    train_loss, test_loss = cross_validate(xgb_x_train, xgb_x_test, xgb_y_train, xgb_y_test, xgb_model)\n",
    "    best_loss = test_loss\n",
    "    \n",
    "    with tqdm(total=len(columns)) as pbar:\n",
    "        for col in columns:\n",
    "            to_del = random.choice(good_cols)\n",
    "            tmp_good_cols = good_cols.copy()\n",
    "            tmp_good_cols.remove(to_del)\n",
    "\n",
    "            X_train_proc = pre_process_data(pd.DataFrame(X_train[tmp_good_cols]))\n",
    "            xgb_x_train, xgb_x_test, xgb_y_train, xgb_y_test = prepare_data(X_train_proc, y_train, test_size=test_size, xgb_format=True)\n",
    "            xgb_model = train_xgb_model(xgb_x_train, params=params, num_round=num_round)\n",
    "            train_loss, test_loss = cross_validate(xgb_x_train, xgb_x_test, xgb_y_train, xgb_y_test, xgb_model)\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                good_cols = tmp_good_cols\n",
    "            pbar.update(1)\n",
    "    return good_cols, best_loss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def prepare_data(x, y, test_size=0.2, xgb_format=True):\n",
    "    if test_size == 0:\n",
    "        dtrain = x\n",
    "        Y_train = y\n",
    "        dtest = None\n",
    "        Y_test = None\n",
    "    else:\n",
    "        dtrain, dtest, Y_train, Y_test = train_test_split(x, y, test_size=test_size, stratify=y, random_state=42)\n",
    "\n",
    "    if xgb_format:\n",
    "        dtrain = xgb.DMatrix(dtrain, label=Y_train)\n",
    "        if dtest is not None:\n",
    "            dtest = xgb.DMatrix(dtest)\n",
    "\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "def train_rf_model(features, labels, **kwargs):\n",
    "\n",
    "    # instantiate model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=0)\n",
    "\n",
    "    # train model\n",
    "    model.fit(features, labels)\n",
    "\n",
    "    # get a (not-very-useful) sense of performance\n",
    "    accuracy = model.score(features, labels)\n",
    "    print(f\"In-sample accuracy: {accuracy:0.2%}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_xgb_model(dtrain, params=None, num_round=100):\n",
    "    if params is None:\n",
    "        params = {'max_depth': 4, 'eta': 100, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    return bst\n",
    "\n",
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Cross Validate\n",
    "def cross_validate(x_train, x_test, y_train, y_test, model):\n",
    "    test_loss = None\n",
    "    if x_test is not None:\n",
    "        preds = model.predict(x_test)\n",
    "        test_loss = log_loss(preds, y_test)\n",
    "\n",
    "    preds_train = model.predict(x_train)\n",
    "    train_loss = log_loss(preds_train, y_train)\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds,\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]\n",
    "\n",
    "\n",
    "def prepare_submission(data_paths, models, enforce_cols=None, to_keep_cols=None, xgb_format=False):\n",
    "    # load test data\n",
    "    a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "    b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "    c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "    if to_keep_cols:\n",
    "        a_test = a_test[to_keep_cols['a']]\n",
    "        b_test = b_test[to_keep_cols['b']]\n",
    "        c_test = c_test[to_keep_cols['c']]\n",
    "\n",
    "    if enforce_cols:\n",
    "        # process the test data\n",
    "        a_test = pre_process_data(a_test, enforce_cols=enforce_cols['a'])\n",
    "        b_test = pre_process_data(b_test, enforce_cols=enforce_cols['b'])\n",
    "        c_test = pre_process_data(c_test, enforce_cols=enforce_cols['c'])\n",
    "\n",
    "    a_test.fillna(0, inplace=True)\n",
    "    b_test.fillna(0, inplace=True)\n",
    "    c_test.fillna(0, inplace=True)\n",
    "\n",
    "    if xgb_format:\n",
    "        a_test = xgb.DMatrix(a_test)\n",
    "        b_test = xgb.DMatrix(b_test)\n",
    "        c_test = xgb.DMatrix(c_test)\n",
    "\n",
    "    # TODO: use probabilities\n",
    "    a_preds = models['a'].predict(a_test)\n",
    "    b_preds = models['b'].predict(b_test)\n",
    "    c_preds = models['c'].predict(c_test)\n",
    "    \n",
    "    a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "    b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "    c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "    \n",
    "    submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Lets filter out the columns with high correlation\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "def get_highly_correlated_columns(df):\n",
    "    df_objs = df.columns.tolist()\n",
    "    df_objs.remove('poor')\n",
    "    corr_matrix = pd.DataFrame(columns=['poor'], index=df_objs)\n",
    "    with tqdm(total=len(df_objs)) as pbar:\n",
    "        for col in df_objs:\n",
    "            confusion_matrix = pd.crosstab(df['poor'], df[col])\n",
    "            corr = cramers_corrected_stat(confusion_matrix)\n",
    "            corr_matrix.loc[col, 'poor'] = corr\n",
    "            pbar.update(1)\n",
    "    return corr_matrix\n",
    "    \"\"\"\n",
    "    cols = {}\n",
    "    for c1 in corr_matrix.columns.tolist():\n",
    "        s = corr_matrix.loc[c1]\n",
    "        s = s[s > 0.5]\n",
    "        s = list(s.index)\n",
    "        s.remove(c1)\n",
    "        cols[c1] = s\n",
    "\n",
    "    cols = {k: cols[k] for k in cols if cols[k]}\n",
    "    return cols\n",
    "    \"\"\"\n",
    "\n",
    "def entropy(a):\n",
    "    return - sum( (a / sum(a)) * np.log((a / sum(a))))\n",
    "\n",
    "\n",
    "def get_entropies(df):\n",
    "    entropies = []\n",
    "    for col in df.columns.tolist():\n",
    "        res = df[col].value_counts()\n",
    "        entropies.append(entropy(res.values))\n",
    "\n",
    "    return entropies\n",
    "\n",
    "\n",
    "def get_low_entropy_columns(df):\n",
    "    to_del = []\n",
    "    entropies = get_entropies(df)\n",
    "    median_entr = np.median(entropies)\n",
    "    #std_entr = np.std(entropies)\n",
    "    #avg_entr = np.mean(entropies)\n",
    "    for i, col in enumerate(df.columns.tolist()):\n",
    "        if entropies[i] < median_entr:\n",
    "            to_del.append(col)\n",
    "    return to_del\n",
    "\n",
    "\n",
    "def filter_columns(df):\n",
    "    to_del = get_low_entropy_columns(df)\n",
    "    print(\"Total columns: {}. To delete: {}\".format(len(df.columns.tolist()), len(to_del)))\n",
    "    to_keep = set(df.columns.tolist()) - set(to_del)\n",
    "    return df[list(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343/343 [1:28:42<00:00, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wBXbHZmp', 'SlDKnCuu', 'KAJOWiiw', 'DsKacCdL', 'rtPrBBPl', 'tMJrvvut', 'jdetlNNF', 'maLAYXwi', 'vwpsXRGk', 'sArDRIyX', 'goxNwvnG', 'TYhoEiNm', 'bgfNZfcj', 'sYIButva', 'VZtBaoXL', 'GUvFHPNA', 'zFkComtB', 'fxbqfEWb', 'HDMHzGif', 'zzwlWZZC', 'nGTepfos', 'DxLvCGgv', 'CbABToOI', 'qgMygRvX', 'uSKnVaKV', 'hESBInAl', 'nzTeWUeM', 'BbKZUYsB', 'UCnazcxd', 'hTraVEWP', 'aCfsveTu', 'EfkPrfXa', 'NrvxpdMQ', 'nEsgxvAq', 'FcekeISI', 'NmAVTtfA', 'YTdCRVJt', 'QyBloWXZ', 'NGOnRdqc', 'HKMQJANN', 'ZRrposmO', 'wakWLjkG', 'vmZttwFZ', 'dkoIJCbY', 'NrUWfvEq', 'WqhniYIc', 'yHbEDILT', 'EJgrQqET', 'nGMEgWyl', 'IIEHQNUc', 'HfKRIwMb', 'NRVuZwXK', 'UCAmikjV', 'UGbBCHRE', 'uJYGhXqG', 'bxKGlBYX', 'nCzVgxgY', 'ltcNxFzI', 'WbEDLWBH', 'IBPMYJlv', 'MxOgekdE', 'ggNglVqE', 'YDgWYWcJ', 'SqGRfEuW', 'WiwmbjGW', 'benRXROb', 'cOSBrarW', 'JwtIxvKg', 'lRGpWehf', 'dSALvhyd', 'gfmfEyjQ', 'WbxAxHul', 'FlBqizNL', 'bEPKkJXP', 'KjkrfGLD', 'JbjHTYUM', 'HmDAlkAH', 'cqUmYeAp', 'sFWbFEso', 'fHUZugEd', 'tZKoAqgl', 'TqrXZaOw', 'galsfNtg', 'VIRwrkXp', 'dsIjcEFe', 'OybQOufM', 'ihGjxdDj', 'gwhBRami', 'FGYOIJbC', 'bPOwgKnT', 'lybuQXPm', 'UBanubTh', 'JeydMEpC', 'fpHOwfAs', 'VXXLUaXP', 'btgWptTG', 'NIRMacrk', 'wxDnGIwN', 'rAkSnhJF', 'EMDSHIlJ', 'bgoWYRMQ', 'glEjrMIg', 'bMudmjzJ', 'GKUhYLAE', 'OnTaJkLa', 'BMmgMRvd', 'OMtioXZZ', 'bIBQTaHw', 'KcArMKAe', 'ofhkZaYa', 'VtcDnwsf', 'TFrimNtw', 'enTUTSQi', 'LjvKYNON', 'wwfmpuWA', 'znHDEHZP', 'kWFVfHWP', 'XwVALSPR', 'TvShZEBA', 'nuwxPLMe', 'eeYoszDM', 'HHAeIHna', 'CrfscGZl', 'dCGNTMiG', 'ngwuvaCV', 'yeHQSlwg', 'XSgHIFXD', 'GnUDarun', 'iwkvfFnL', 'ANBCxZzU', 'NanLCXEI', 'SqEqFZsM', 'ZnBLVaqz', 'lQQeVmCa', 'lFcfBRGd', 'MOIscfCf', 'AsEmHUzj', 'YXkrVgqt', 'pyBSpOoN', 'srPNUgVy', 'UXSJUVwD', 'pCgBHqsR', 'wEbmsuJO', 'TWXCrjor', 'wgWdGBOp', 'mRgnuJVE', 'pWyRKfsb', 'udzhtHIr', 'ErggjCIN', 'iVscWZyL', 'IZFarbPw', 'lnfulcWk', 'UHGnBrNt', 'QNLOXNwj', 'ytYMzOlW', 'YFMZwKrU', 'RJQbcmKy', 'uizuNzbk', 'dlyiMEQt', 'ucXrHdoC', 'iBQXwnGC', 'TnWhKowI', 'sslNoPlw', 'InULRrrv', 'LoYIbglA', 'GhJKwVWC', 'zXPyHBkn', 'lVHmBCmb', 'EuJrVjyG', 'nSzbETYS', 'CpqWSQcW', 'jxSUvflR', 'XqURHMoh', 'eoNxXdlZ', 'qgxmqJKa', 'mDTcQhdH', 'gfurxECf', 'RLKqBexO', 'mvGdZZcs', 'duayPuvk', 'hnrnuMte', 'CbzSWtkF', 'XDDOZFWf', 'CIGUXrRQ', 'ccAHraiP', 'QayGNSmS', 'CtFxPQPT', 'ePtrWTFd', 'tbsBPHFD', 'naDKOzdk', 'lTAXSTys', 'DNAfxPzs', 'GYTJWlaF', 'xkUFKUoW', 'jVDpuAmP', 'mnIQKNOM', 'bhFgAObo', 'SeZULMCT', 'AtGRGAYi', 'NitzgUzY', 'YlZCqMNw', 'FGDcbVBN', 'rYvVKPAF', 'WTFJilSZ', 'NBfffJUe', 'wnESwOiN', 'rfDBJtIz', 'mvgxfsRb', 'BwkgSxCk', 'KHzKOKPw', 'UXfyiodk', 'EftwspgZ', 'pQmBvlkz', 'mycoyYwl', 'ySkAFOzx', 'dkPWxwSF', 'bSaLisbO', 'wKcZtLNv', 'mBlWbDmc', 'szowPwNq', 'ULMvnWcn', 'BfGjiYom', 'iWEFJYkR', 'ogHwwdzc', 'BCehjxAl', 'CHAQHqqr', 'nqndbwXP', 'uZGqTQUP', 'phwExnuQ', 'SzUcfjnr', 'PXtHzrqw', 'CNkSTLvx', 'tHFrzjai', 'MKozKLvT', 'pjHvJhoZ', 'zkbPtFyO', 'xZBEXWPR', 'dyGFeFAg', 'HfOrdgBo', 'pKPTBZZq', 'bCYWWTxH', 'EQKKRGkR', 'ZmJZXnoA', 'YKwvJgoP', 'dAaIakDk', 'rnJOTwVD', 'xNUUjCIL', 'JMNvdasy', 'MBQcYnjc', 'cCsFudxF', 'muIetHMK', 'hJrMTBVd', 'TJUYOoXU', 'ishdUooQ', 'tjrOpVkX', 'cWNZCMRB', 'cgJgOfCA', 'ItpCDLDM', 'gOGWzlYC', 'ptEAnCSs', 'HDCjCTRd', 'lOujHrCk', 'MARfVwUE', 'orfSPOJX', 'OKMtkqdQ', 'qTginJts', 'JzhdOhzb', 'THDtJuYh', 'QqoiIXtI', 'XVwajTfe', 'jwEuQQve', 'rQAsGegu', 'nKoaotpH', 'kLkPtNnh', 'TzPdCEPV', 'CtHqaXhY', 'DbUNVFwv', 'FmSlImli', 'UsENDgsH', 'TiwRslOh', 'mNrEOmgq', 'PWShFLnY', 'uRFXnNKV', 'lFExzVaF', 'CVCsOVew', 'tlxXCDiW', 'IKqsuNvV', 'ztGMreNV', 'CqqwKRSn', 'YUExUvhq', 'uVnApIlJ', 'fsXLyyco', 'UXhTXbuS', 'yaHLJxDD', 'qlZMvcWc', 'dqRtXzav', 'ALbGNqKm', 'ktBqxSwa', 'NqPjMmKP', 'tOWnWxYe', 'RvTenIlS', 'GIMIxlmv', 'wKVwRQIp', 'ncjMNgfp', 'UaXLYMMh', 'vRIvQXtC', 'bKtkhUWD', 'RJFKdmYJ', 'gllMXToa', 'HhKXJWno', 'VFTkSOrq', 'tAYCAXge', 'WAFKMNwv', 'mHzqKSuN', 'UjuNwfjv', 'aWlBVrkK', 'cDkXTaWP', 'hnmsRSvN', 'ZzUrQSMj', 'GHmAeUhZ', 'VBjVVDwp', 'RMbjnrlm', 'kZVpcgJL', 'zvjiUrCR', 'BIofZdtd', 'QZiSWCCB', 'sDGibZrP', 'CsGvKKBJ', 'OLpGAaEu', 'LrDrWRjC', 'JCDeZBXq', 'HGPWuGlV', 'GDUPaBQs', 'WuwrCsIY', 'AlDbXTlZ'] 2.996337435017126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Country A\")\n",
    "params = {'max_depth': 9, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "num_round = 500\n",
    "good_cols, test_loss = get_best_columns(a_train, params=params, num_round=num_round)\n",
    "print(good_cols, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 82/440 [24:47<1:44:43, 17.55s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Country B\")\n",
    "params = {'max_depth': 9, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "num_round = 500\n",
    "good_cols_b, test_loss_b = get_best_columns(b_train, params=params, num_round=num_round)\n",
    "print(good_cols_b, test_loss_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Total columns: 344. To delete: 172\n",
      "Total columns: 344. To delete: 172\n",
      "172 172\n",
      "{'wwfmpuWA'}\n",
      "\n",
      "Country B\n",
      "Total columns: 441. To delete: 220\n",
      "Total columns: 441. To delete: 220\n",
      "221 221\n",
      "{'zCnhAreR', 'EmHAsgcA', 'xjaMthYM'}\n",
      "\n",
      "Country C\n",
      "Total columns: 163. To delete: 81\n",
      "Total columns: 163. To delete: 81\n",
      "82 82\n",
      "{'WWuPOkor', 'wlNGOnRd'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry C\")\n",
    "c_test_reduc = filter_columns(c_test)\n",
    "c_train_reduc = filter_columns(c_train.drop('poor', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with columns with more entropy (Submitted Jan 8 2018. Score:  0.24977 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to repreprocess the data with less columns\n",
    "a_train_reduc = a_train[a_test_reduc.columns.tolist() + ['poor']]\n",
    "b_train_reduc = b_train[b_test_reduc.columns.tolist() + ['poor']]\n",
    "c_train_reduc = c_train[c_test_reduc.columns.tolist() + ['poor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [00:00<00:00, 190.13it/s]\n",
      "100%|██████████| 221/221 [00:01<00:00, 200.43it/s]\n",
      " 40%|████      | 33/82 [00:00<00:00, 148.11it/s]/home/eze/.miniconda3/envs/poverty/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "100%|██████████| 82/82 [00:00<00:00, 161.12it/s]\n"
     ]
    }
   ],
   "source": [
    "a_corr_cols = get_highly_correlated_columns(a_train_reduc)\n",
    "b_corr_cols = get_highly_correlated_columns(b_train_reduc)\n",
    "c_corr_cols = get_highly_correlated_columns(c_train_reduc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['QyBloWXZ', 'poor']\n",
      "['lCKzGQow', 'DwxXAlcv', 'frkmPrFd', 'BjWMmVMX', 'qnCnHAnk', 'UEaRhdUa', 'TChiqwQp', 'qrOrXLPM', 'plRFsRMw', 'poor']\n",
      "['DBjxSUvf', 'tFrTiLjv', 'gZWEypOM', 'qCEuAGDU', 'CBoRtiUy', 'GIwNbAsH', 'mmoCpqWS', 'VbnOIDkC', 'LhUIIEHQ', 'BBPluVrb', 'snqZfFGY', 'XsbpBUGN', 'gLDyDXsb', 'laWlBVrk', 'xyzchLjk', 'nomHWXYi', 'wcNjwEuQ', 'YmHrcUIw', 'MtkqdQSs', 'kZmWbEDL', 'kiAJBGqv', 'YACFXGNR', 'kdkPWxwS', 'qbMphwEx', 'nRXRObKS', 'poor']\n"
     ]
    }
   ],
   "source": [
    "a_useful_cols = a_corr_cols[a_corr_cols['poor'] > 0.4].index.tolist() + ['poor']\n",
    "print(a_useful_cols)\n",
    "b_useful_cols = b_corr_cols[b_corr_cols['poor'] > 0.2].index.tolist() + ['poor']\n",
    "print(b_useful_cols)\n",
    "c_useful_cols = c_corr_cols[c_corr_cols['poor'] > 0.25].index.tolist() + ['poor']\n",
    "print(c_useful_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed from A: 343\n",
      "Columns removed from B: 432\n",
      "Columns removed from C: 138\n"
     ]
    }
   ],
   "source": [
    "a_train_small = a_train_reduc[a_useful_cols]\n",
    "b_train_small = b_train_reduc[b_useful_cols]\n",
    "c_train_small = c_train_reduc[c_useful_cols]\n",
    "\n",
    "\n",
    "print(\"Columns removed from A: {}\".format(len(a_train.columns) - len(a_train_small.columns)))\n",
    "print(\"Columns removed from B: {}\".format(len(b_train.columns) - len(b_train_small.columns)))\n",
    "print(\"Columns removed from C: {}\".format(len(c_train.columns) - len(c_train_small.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(8203, 2)\n",
      "After standardization (8203, 2)\n",
      "After converting categoricals:\t(8203, 3)\n",
      "Input shape:\t(3255, 10)\n",
      "After standardization (3255, 10)\n",
      "After converting categoricals:\t(3255, 90)\n",
      "Input shape:\t(6469, 26)\n",
      "After standardization (6469, 26)\n",
      "After converting categoricals:\t(6469, 219)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/.miniconda3/envs/poverty/lib/python3.6/site-packages/pandas/core/frame.py:2352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "aX_train = pre_process_data(a_train_small)\n",
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "\n",
    "bX_train = pre_process_data(b_train_small)\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "\n",
    "cX_train = pre_process_data(c_train_small)\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.\n",
    "\n",
    "xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test = prepare_data(aX_train, ay_train, test_size=test_size, xgb_format=True)\n",
    "xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test = prepare_data(bX_train, by_train, test_size=test_size, xgb_format=True)\n",
    "xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test = prepare_data(cX_train, cy_train, test_size=test_size, xgb_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 3000\n",
    "params = {'max_depth': 9, 'eta': 0.05, 'silent': 0, 'lambda': 0.5, 'alpha': 0.5, 'lambda_bias': 0.5, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "\n",
    "xgb_a = train_xgb_model(xgb_ax_train, params=params, num_round=num_round)\n",
    "xgb_b = train_xgb_model(xgb_bx_train, params=params, num_round=num_round)\n",
    "xgb_c = train_xgb_model(xgb_cx_train, params=params, num_round=num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Loss. Train: 0.008615946642599797 - Test: None\n",
      "B Loss. Train: 0.026407453189422843 - Test: None\n",
      "C Loss. Train: 0.01067026217238942 - Test: None\n"
     ]
    }
   ],
   "source": [
    "print(\"A Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_ax_train, xgb_ax_test, xgb_ay_train, xgb_ay_test, xgb_a)))\n",
    "print(\"B Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_bx_train, xgb_bx_test, xgb_by_train, xgb_by_test, xgb_b)))\n",
    "print(\"C Loss. Train: {} - Test: {}\".format(*cross_validate(xgb_cx_train, xgb_cx_test, xgb_cy_train, xgb_cy_test, xgb_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 1)\n",
      "After standardization (4041, 1)\n",
      "After converting categoricals:\t(4041, 2)\n",
      "Input shape:\t(1604, 9)\n",
      "After standardization (1604, 9)\n",
      "After converting categoricals:\t(1604, 89)\n",
      "Input shape:\t(3187, 25)\n",
      "After standardization (3187, 25)\n",
      "After converting categoricals:\t(3187, 208)\n",
      "(4041, 3)\n",
      "(1604, 90)\n",
      "(3187, 219)\n"
     ]
    }
   ],
   "source": [
    "# Prepare submission\n",
    "models = {'a': xgb_a, 'b': xgb_b, 'c': xgb_c}\n",
    "a_keep = a_train_small.columns.tolist()\n",
    "a_keep.remove('poor')\n",
    "b_keep = b_train_small.columns.tolist()\n",
    "b_keep.remove('poor')\n",
    "c_keep = c_train_small.columns.tolist()\n",
    "c_keep.remove('poor')\n",
    "\n",
    "#to_keep_cols = {'a': a_keep, 'b': b_keep, 'c': c_keep}\n",
    "#enforce_cols = {'a': a_keep, 'b': b_keep, 'c': c_keep}\n",
    "\n",
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "a_test = a_test[a_keep]\n",
    "b_test = b_test[b_keep]\n",
    "c_test = c_test[c_keep]\n",
    "\n",
    "\n",
    "a_test = pre_process_data(a_test)\n",
    "b_test = pre_process_data(b_test)\n",
    "c_test = pre_process_data(c_test)\n",
    "\n",
    "# Delete new columns that were not in training set\n",
    "a_diff = set(a_test.columns.tolist()) - set(aX_train.columns.tolist())\n",
    "b_diff = set(b_test.columns.tolist()) - set(bX_train.columns.tolist())\n",
    "c_diff = set(c_test.columns.tolist()) - set(cX_train.columns.tolist())\n",
    "\n",
    "a_test = a_test[a_test.columns.difference(list(a_diff))]\n",
    "b_test = b_test[b_test.columns.difference(list(b_diff))]\n",
    "c_test = c_test[c_test.columns.difference(list(c_diff))]\n",
    "\n",
    "# Add dummy columns that are not in the test set\n",
    "a_diff = set(aX_train.columns.tolist()) - set(a_test.columns.tolist())\n",
    "b_diff = set(bX_train.columns.tolist()) - set(b_test.columns.tolist())\n",
    "c_diff = set(cX_train.columns.tolist()) - set(c_test.columns.tolist())\n",
    "a_test = a_test.assign(**{c: 0 for c in a_diff})\n",
    "b_test = b_test.assign(**{c: 0 for c in b_diff})\n",
    "c_test = c_test.assign(**{c: 0 for c in c_diff})\n",
    "\n",
    "# Reorder columns in the original way so XGBoost does not explode\n",
    "a_test = a_test[aX_train.columns.tolist()]\n",
    "b_test = b_test[bX_train.columns.tolist()]\n",
    "c_test = c_test[cX_train.columns.tolist()]\n",
    "\n",
    "\n",
    "a_test.fillna(0, inplace=True)\n",
    "b_test.fillna(0, inplace=True)\n",
    "c_test.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(a_test.shape)\n",
    "print(b_test.shape)\n",
    "print(c_test.shape)\n",
    "\n",
    "a_testxgb = xgb.DMatrix(a_test)\n",
    "b_testxgb = xgb.DMatrix(b_test)\n",
    "c_testxgb = xgb.DMatrix(c_test)\n",
    "\n",
    "# TODO: use probabilities\n",
    "a_preds = xgb_a.predict(a_testxgb)\n",
    "b_preds = xgb_b.predict(b_testxgb)\n",
    "c_preds = xgb_c.predict(c_testxgb)\n",
    "\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')\n",
    "\n",
    "submission = pd.concat([a_sub, b_sub, c_sub])\n",
    "\n",
    "#submission = prepare_submission(data_paths, models, enforce_cols=enforce_cols, to_keep_cols=to_keep_cols, xgb_format=True)\n",
    "submission.to_csv('submission_recent_XGB_best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
