{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from tqdm import tqdm\n",
    "import scipy as sc\n",
    "import scipy.stats as ss\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('../..', 'data')\n",
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}\n",
    "\n",
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x, y, test_size=0.2):\n",
    "    objs = x.select_dtypes(include=['O'])\n",
    "    objs = objs.columns.tolist()\n",
    "    \n",
    "    for col in objs:\n",
    "        le = LabelEncoder()\n",
    "        x[col] = le.fit_transform(list(x[col]))\n",
    "                                  \n",
    "    if test_size == 0:\n",
    "        dtrain = x\n",
    "        Y_train = y\n",
    "        dtest = None\n",
    "        Y_test = None\n",
    "    else:\n",
    "        dtrain, dtest, Y_train, Y_test = train_test_split(x, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Cross Validate\n",
    "def cross_validate(x_train, x_test, y_train, y_test, model):\n",
    "    test_loss = None\n",
    "    if x_test is not None:\n",
    "        preds = model.predict(x_test)\n",
    "        test_loss = log_loss(preds, y_test)\n",
    "\n",
    "    preds_train = model.predict(x_train, num_iteration=model.best_iteration)\n",
    "    train_loss = log_loss(preds_train, y_train)\n",
    "    return train_loss, test_loss\n",
    "\n",
    "# Lets filter out the columns with high correlation\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "def get_highly_correlated_columns(df):\n",
    "    df_objs = df.select_dtypes(include=['O'])\n",
    "    #del df_objs['country']\n",
    "    corr_matrix = pd.DataFrame(columns=df_objs.columns, index=df_objs.columns.tolist())\n",
    "    with tqdm(total=len(corr_matrix.columns.tolist()) * len(corr_matrix.columns.tolist())) as pbar:\n",
    "        for col1 in df_objs.columns.tolist():\n",
    "            for col2 in df_objs.columns.tolist():\n",
    "                if col1 != col2:\n",
    "                    confusion_matrix = pd.crosstab(df_objs[col1], df_objs[col2])\n",
    "                    corr = cramers_corrected_stat(confusion_matrix)\n",
    "                else:\n",
    "                    corr = 1\n",
    "                corr_matrix.loc[col1, col2] = corr\n",
    "                pbar.update(1)\n",
    "    \n",
    "    cols = {}\n",
    "    for c1 in corr_matrix.columns.tolist():\n",
    "        s = corr_matrix.loc[c1]\n",
    "        s = s[s > 0.5]\n",
    "        s = list(s.index)\n",
    "        s.remove(c1)\n",
    "        cols[c1] = s\n",
    "\n",
    "    cols = {k: cols[k] for k in cols if cols[k]}\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train.poor.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "aX_train = a_train.drop(['poor', 'country'], axis=1)\n",
    "\n",
    "b_train.poor.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "bX_train = b_train.drop(['poor', 'country'], axis=1)\n",
    "\n",
    "c_train.poor.fillna(False, inplace=True)\n",
    "cy_train = np.ravel(c_train.poor.astype(int))\n",
    "cX_train = c_train.drop(['poor', 'country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114921/114921 [10:48<00:00, 177.29it/s]\n",
      "  0%|          | 0/173889 [00:00<?, ?it/s]/home/eze/.miniconda3/envs/poverty/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 173889/173889 [15:26<00:00, 187.61it/s]\n",
      "100%|██████████| 17424/17424 [01:34<00:00, 183.99it/s]\n"
     ]
    }
   ],
   "source": [
    "a_corr_cols = get_highly_correlated_columns(aX_train)\n",
    "b_corr_cols = get_highly_correlated_columns(bX_train)\n",
    "c_corr_cols = get_highly_correlated_columns(cX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed from A: 103\n",
      "Columns removed from B: 183\n",
      "Columns removed from C: 75\n"
     ]
    }
   ],
   "source": [
    "# Lets delete the highly correlated columns in a greedy manner\n",
    "def delete_columns(df, dict_cols):\n",
    "    to_delete = list(set([elem for values in dict_cols.values() for elem in values]))\n",
    "    return df.drop(to_delete, axis=1)\n",
    "\n",
    "\n",
    "a_train_small = delete_columns(aX_train, a_corr_cols)\n",
    "b_train_small = delete_columns(bX_train, b_corr_cols)\n",
    "c_train_small = delete_columns(cX_train, c_corr_cols)\n",
    "\n",
    "\n",
    "print(\"Columns removed from A: {}\".format(len(aX_train.columns) - len(a_train_small.columns)))\n",
    "print(\"Columns removed from B: {}\".format(len(bX_train.columns) - len(b_train_small.columns)))\n",
    "print(\"Columns removed from C: {}\".format(len(cX_train.columns) - len(c_train_small.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "a_train, a_test, ay_train, ay_test = prepare_data(a_train_small, ay_train, test_size=test_size)\n",
    "b_train, b_test, by_train, by_test = prepare_data(b_train_small, by_train, test_size=test_size)\n",
    "c_train, c_test, cy_train, cy_test = prepare_data(c_train_small, cy_train, test_size=test_size)\n",
    "\n",
    "test_size = 0.5\n",
    "a_val, a_test, ay_val, ay_test = prepare_data(a_test, ay_test, test_size=test_size)\n",
    "b_val, b_test, by_val, by_test = prepare_data(b_test, by_test, test_size=test_size)\n",
    "c_val, c_test, cy_val, cy_test = prepare_data(c_test, cy_test, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eze/.miniconda3/envs/poverty/lib/python3.6/site-packages/lightgbm/basic.py:1029: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is []\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model...\n",
      "Start predicting...\n",
      "The rmse of prediction is: 0.3231831485298325\n",
      "A Loss. Train: 3.550185593369918 - Test: 7.818836821953888\n"
     ]
    }
   ],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(a_train, ay_train)\n",
    "lgb_eval = lgb.Dataset(a_val, ay_val, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'logloss'},#, 'l2'},\n",
    "    'num_leaves': 500,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "categ = a_train_small.select_dtypes(include=['O'])\n",
    "categ = categ.columns.tolist()\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=lgb_eval,\n",
    "                #early_stopping_rounds=100,\n",
    "                #feature_name=a_train.columns.tolist(),\n",
    "                categorical_feature=categ)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(a_test, num_iteration=gbm.best_iteration)\n",
    "y_pred_tr = gbm.predict(a_train, num_iteration=gbm.best_iteration)\n",
    "\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(ay_test, y_pred) ** 0.5)\n",
    "train_loss, test_loss = cross_validate(a_train, a_test, ay_train, ay_test, gbm)\n",
    "print(\"A Loss. Train: {} - Test: {}\".format(train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
