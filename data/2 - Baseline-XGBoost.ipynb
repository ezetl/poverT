{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import scipy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = os.path.join('..', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = {'A': {'train': os.path.join(DATA_DIR, 'A_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'A_hhold_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(DATA_DIR, 'B_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(DATA_DIR, 'C_hhold_train.csv'), \n",
    "                    'test':  os.path.join(DATA_DIR, 'C_hhold_test.csv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'], index_col='id')\n",
    "b_train = pd.read_csv(data_paths['B']['train'], index_col='id')\n",
    "c_train = pd.read_csv(data_paths['C']['train'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country A\n",
      "Input shape:\t(1855, 344)\n",
      "After standardization (1855, 344)\n",
      "After converting categoricals:\t(1855, 849)\n",
      "Shapes: aX_train (1855, 849) - ay_train (1855,)\n",
      "\n",
      "Country B\n",
      "Input shape:\t(3255, 441)\n",
      "After standardization (3255, 441)\n",
      "After converting categoricals:\t(3255, 1432)\n",
      "Shapes: bX_train (3255, 1432) - by_train (3255,)\n",
      "\n",
      "Country C\n",
      "Input shape:\t(6469, 163)\n",
      "After standardization (6469, 163)\n",
      "After converting categoricals:\t(6469, 795)\n",
      "Shapes: cX_train (6469, 795) - cy_train (6469,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Country A\")\n",
    "aX_train = pre_process_data(a_train.drop('poor', axis=1))\n",
    "a_train.fillna(False, inplace=True)\n",
    "ay_train = np.ravel(a_train.poor.astype(int))\n",
    "print(\"Shapes: aX_train {} - ay_train {}\".format(aX_train.shape, ay_train.shape))\n",
    "\n",
    "print(\"\\nCountry B\")\n",
    "bX_train = pre_process_data(b_train.drop('poor', axis=1))\n",
    "b_train.fillna(False, inplace=True)\n",
    "by_train = np.ravel(b_train.poor.astype(int))\n",
    "print(\"Shapes: bX_train {} - by_train {}\".format(bX_train.shape, by_train.shape))\n",
    "\n",
    "print(\"\\nCountry C\")\n",
    "cX_train = pre_process_data(c_train.drop('poor', axis=1))\n",
    "cy_train = np.ravel(c_train.poor.astype(int))\n",
    "print(\"Shapes: cX_train {} - cy_train {}\".format(cX_train.shape, cy_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x, y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "    dtrain = xgb.DMatrix(X_train, label=Y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    return dtrain, dtest, Y_train, Y_test\n",
    "\n",
    "\n",
    "def train_model(dtrain, params=None, num_round=100):\n",
    "    if params is None:\n",
    "        params = {'max_depth': 4, 'eta': 100, 'silent': 1, 'objective': 'reg:logistic'}\n",
    "\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dtrain, a_dtest, ay_train, ay_test = prepare_data(aX_train, ay_train)\n",
    "b_dtrain, b_dtest, by_train, by_test = prepare_data(bX_train, by_train)\n",
    "c_dtrain, c_dtest, cy_train, cy_test = prepare_data(cX_train, cy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "# -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "def log_loss(yt, yp):\n",
    "    # yt: groundtruth\n",
    "    # yp: predicted\n",
    "    ground = np.array(yt)\n",
    "    pred = yp.astype(float)\n",
    "    eps_pred = np.maximum(np.minimum(pred, 1. - 1e-15), 1e-15)\n",
    "    loss = -(ground * np.log(eps_pred) + (1 - ground) * np.log(1 - eps_pred))\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the optimal hyperparameters\n",
    "def tune_params(dtrain, dtest, y_test):\n",
    "    params = {'max_depth': 4, 'eta': 0.1, 'silent': 1, 'lambda': 1, 'min_child_weight': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42}\n",
    "    current_loss = 10000\n",
    "    for num_round in [10, 50, 100]:\n",
    "        for max_depth in range(3, 10):\n",
    "            for eta in [0.01, 0.05, 0.09, 0.1]:\n",
    "                for min_child_weight in [1, 2, 5]:\n",
    "                    for lamda in [0.5, 1]:\n",
    "                        params['max_depth'] = max_depth\n",
    "                        params['eta'] = eta\n",
    "                        params['min_child_weight'] = min_child_weight\n",
    "                        params['lambda'] = lamda\n",
    "\n",
    "                        model = train_model(dtrain, params=params, num_round=num_round)\n",
    "\n",
    "                        pred = model.predict(dtest)\n",
    "                        loss = log_loss(pred, y_test)\n",
    "\n",
    "                        if loss < current_loss:\n",
    "                            current_loss = loss\n",
    "                            best_hyperparams = params\n",
    "                            best_num_rounds = num_round\n",
    "\n",
    "    return best_hyperparams, best_num_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 504) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuning parameters for Country A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (504 of 504) |#######################| Elapsed Time: 0:13:49 Time: 0:13:49\n",
      "N/A% (0 of 504) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuning parameters for Country B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (1 of 504) |          | Elapsed Time: 0:01:49 ETA:  73231508 days, 0:30:00"
     ]
    }
   ],
   "source": [
    "# Test set\n",
    "print(\"\\n\\nTuning parameters for Country A\")\n",
    "a_params, a_num_rounds = tune_params(a_dtrain, a_dtest, ay_test)\n",
    "print(\"\\n\\nTuning parameters for Country B\")\n",
    "b_params, b_num_rounds = tune_params(b_dtrain, b_dtest, by_test)\n",
    "print(\"\\n\\nTuning parameters for Country C\")\n",
    "c_params, c_num_rounds = tune_params(c_dtrain, c_dtest, cy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2681683632\n",
      "32.6287979768\n",
      "29.7343098176\n"
     ]
    }
   ],
   "source": [
    "# Final training\n",
    "a_params['silent'] = 0\n",
    "b_params['silent'] = 0\n",
    "c_params['silent'] = 0\n",
    "model_a = train_model(a_dtrain, params=a_params, num_rounds=a_num_rounds)\n",
    "model_b = train_model(b_dtrain, params=b_params, num_rounds=b_num_rounds)\n",
    "model_c = train_model(c_dtrain, params=c_params, num_rounds=c_num_rounds)\n",
    "\n",
    "a_pred = model_a.predict(a_dtest)\n",
    "a_pred_train = model_a.predict(a_dtrain)\n",
    "\n",
    "b_pred = model_b.predict(b_dtest)\n",
    "b_pred_train = model_b.predict(b_dtrain)\n",
    "\n",
    "c_pred = model_c.predict(c_dtest)\n",
    "c_pred_train = model_c.predict(c_dtrain)\n",
    "\n",
    "print(\"Loss A Test: {} - Train: {}\".format(log_loss(a_pred, ay_test), log_loss(a_pred_train, ay_train)))\n",
    "print(\"Loss B Test: {} - Train: {}\".format(log_loss(b_pred, by_test), log_loss(b_pred_train, by_train)))\n",
    "print(\"Loss C Test: {} - Train: {}\".format(log_loss(c_pred, cy_test), log_loss(c_pred_train, cy_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 344)\n",
      "After standardization (4041, 344)\n",
      "After converting categoricals:\t(4041, 851)\n",
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n",
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n"
     ]
    }
   ],
   "source": [
    "# process the test data\n",
    "a_test = pre_process_data(a_test, enforce_cols=aX_train.columns)\n",
    "b_test = pre_process_data(b_test, enforce_cols=bX_train.columns)\n",
    "c_test = pre_process_data(c_test, enforce_cols=cX_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_preds = model_a.predict(a_test)\n",
    "b_preds = model_b.predict(b_test)\n",
    "c_preds = model_c.predict(c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds[:, 1],  # proba p=1\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert preds to data frames\n",
    "a_sub = make_country_sub(a_preds, a_test, 'A')\n",
    "b_sub = make_country_sub(b_preds, b_test, 'B')\n",
    "c_sub = make_country_sub(c_preds, c_test, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([a_sub, b_sub, c_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
